{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Week 7: Parameter Estimation Methods**\n",
        "\n",
        "```\n",
        ".------------------------------------.\n",
        "|   __  ____  ______  _  ___ _____   |\n",
        "|  |  \\/  \\ \\/ / __ )/ |/ _ \\___  |  |\n",
        "|  | |\\/| |\\  /|  _ \\| | | | | / /   |\n",
        "|  | |  | |/  \\| |_) | | |_| |/ /    |\n",
        "|  |_|  |_/_/\\_\\____/|_|\\___//_/     |\n",
        "'------------------------------------'\n",
        "\n",
        "```\n",
        "\n",
        "Through various exercises, the first part of this workshop will review two common point estimation methods: the method of moments and maximum likelihood estimation. In the second part, we will focus on interval estimation using the asymptotic distribution of sample means."
      ],
      "metadata": {
        "id": "ZtpJJ8BSHOaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-Configurating the Notebook**"
      ],
      "metadata": {
        "id": "SVtkEkRDY5Ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Switching to the R Kernel on Colab**\n",
        "\n",
        "By default, Google Colab uses Python as its programming language. To use R instead, you’ll need to manually switch the kernel by going to **Runtime > Change runtime type**, and selecting R as the kernel. This allows you to run R code in the Colab environment.\n",
        "\n",
        "However, our notebook is already configured to use R by default. Unless something goes wrong, you shouldn’t need to manually change runtime type."
      ],
      "metadata": {
        "id": "0vfwHxobY6_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing Required Packages**\n",
        "**Run the following lines of code**:"
      ],
      "metadata": {
        "id": "36rWeG2RY7fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do not modify\n",
        "\n",
        "setwd(\"/content\")\n",
        "\n",
        "# Remove `MXB107-Notebooks` if exists,\n",
        "if (dir.exists(\"MXB107-Notebooks\")) {\n",
        "  system(\"rm -rf MXB107-Notebooks\")\n",
        "}\n",
        "\n",
        "# Fork the repository\n",
        "system(\"git clone https://github.com/edelweiss611428/MXB107-Notebooks.git\")\n",
        "\n",
        "# Change working directory to \"MXB107-Notebooks\"\n",
        "setwd(\"MXB107-Notebooks\")\n",
        "\n",
        "#\n",
        "invisible(source(\"R/preConfigurated.R\"))"
      ],
      "metadata": {
        "id": "t_2d_ItyY9yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do not modify the following**"
      ],
      "metadata": {
        "id": "q-ITFB1qY_ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if (!require(\"testthat\")) install.packages(\"testthat\"); library(\"testthat\")\n",
        "\n",
        "test_that(\"Test if all packages have been loaded\", {\n",
        "\n",
        "  expect_true(all(c(\"ggplot2\", \"tidyr\", \"dplyr\", \"stringr\", \"magrittr\", \"knitr\") %in% loadedNamespaces()))\n",
        "\n",
        "})"
      ],
      "metadata": {
        "id": "Hgh75IDpZCf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Point Estimation**\n",
        "\n",
        "We have studied point estimation before, where our goal was to estimate population parameters such as the mean ($\\mu$) and variance ($\\sigma^2$). In those cases, we used the **sample mean** ($\\bar{x}$)  and **sample variance** ($s^2$) as our estimators. Each of these provides a single number from the data that serves as our “best guess” for the true population parameter.\n",
        "\n",
        "- If the population mean is $\\mu$, then the sample mean $\\bar{x}$ is our point estimate.\n",
        "- If the population variance is $\\sigma^2$, then the sample variance $s^2$ is our point estimate.\n",
        "\n",
        "These estimators also have desirable properties: they are unbiased — ***on average***, they hit the true parameter; and they become increasingly \"accurate\" as the sample size grows.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DM1B0p44cnem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More generally, we consider the situation where the sample data $\\{x_1, \\dots, x_n\\}$ are generated according to a **statistical model** with probability distribution  \n",
        "\n",
        "$$\n",
        "f(\\{x_1, \\dots, x_n\\} \\mid \\theta),\n",
        "$$\n",
        "\n",
        "where $\\theta$ represents one or more unknown parameters of interest. Given the observed data, our goal is to construct an estimator $\\hat{\\theta}$ that provides a single “best guess” for the true value of $\\theta$.  \n",
        "\n",
        "Often, we assume that the data $\\{x_1, \\dots, x_n\\}$ are realisations of i.i.d. random variables with probability function $p(x \\mid \\theta)$. In that case, the joint probability can be written as  \n",
        "\n",
        "$$\n",
        "f(\\{x_1, \\dots, x_n\\} \\mid \\theta) = \\prod_{i=1}^{n} p(x_i \\mid \\theta).\n",
        "$$  \n",
        "\n",
        "Note that, **pre-sample** (i.e. before observing any data), like sample mean and sample variance, $\\hat{\\theta}$ is also a random variable as it is a function of random variables (sample data). The point estimate computed after observing the sample data is a realisation of this random variable. Although we have explored the sampling distribution of sample staistics such as the sample mean, details about the sampling distribution of $\\hat{\\theta}$ generally are beyond the scope of this unit.\n"
      ],
      "metadata": {
        "id": "N81cpLdOYX9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will introduce two fundamental methods for constructing such estimators: the **method of moments (MoM)** and **maximum likelihood estimation (MLE)**. Both methods start from the same observed data but approach the estimation problem in different ways. Our main focus will be on understanding how these methods are formulated and applied in practice.\n",
        "\n",
        "**Note:** All estimation techniques we learn in this unit assume that the data are generated from a particular statistical model. In practice, this assumption might not strictly hold. Often, we have several competing models, and we need to estimate parameters for each model before selecting the best one. Learning to estimate model paramaeters is therefore an essential first step in a broader modelling workflow."
      ],
      "metadata": {
        "id": "SlJkgZvNGW25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Method of Moments**\n",
        "\n",
        "The method of moments is a simple way to construct estimators by equating sample moments with theoretical moments. Suppose our model has an unknown parameter $\\theta$, and let $m_k(\\theta)$ denote the $k$-th population moment:\n",
        "\n",
        "$$\n",
        "m_k(\\theta) = \\mathbb{E}[X^k].\n",
        "$$\n",
        "\n",
        "From a sample $x_1, \\dots, x_n$, we can compute the corresponding sample moments:\n",
        "\n",
        "$$\n",
        "\\hat{m}_k = \\frac{1}{n} \\sum_{i=1}^{n} x_i^k.\n",
        "$$\n",
        "\n",
        "The idea of MoM is to solve the system of equations\n",
        "\n",
        "$$\n",
        "\\hat{m}_k = m_k(\\theta), \\quad k = 1, 2, \\dots\n",
        "$$\n",
        "\n",
        "for the unknown parameter(s) $\\theta$. The solution gives the method of moments estimator $\\hat{\\theta}_{\\text{MoM}}$.  In general, for a model with $k$ parameters, we need to solve $k$ equations.\n",
        "\n",
        "MoM is intuitive and easy to compute. However, it may produce nonsensical values (e.g., negative estimates for parameters that must be positive) if the sample moments deviate strongly from theoretical constraints."
      ],
      "metadata": {
        "id": "J0ptx2-jpo9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Maximum Likelihood Estimation**\n",
        "\n",
        "Maximum likelihood estimation takes a different approach. Suppose the sample $x_1, \\dots, x_n$ are realisations from i.i.d. random variables with probability function $p(x \\mid \\theta)$. The **likelihood function** is\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\prod_{i=1}^{n} p(x_i \\mid \\theta),\n",
        "$$\n",
        "\n",
        "which we can also write in log form as\n",
        "\n",
        "$$\n",
        "\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} \\log p(x_i \\mid \\theta).\n",
        "$$\n",
        "\n",
        "\n",
        "Note that the likelihood function is a function of the model parameter $\\theta$, while the sample data $\\{x_1, \\dots, x_n\\}$ are fixed. It represents how plausible the observed data are for each possible value of $\\theta$.\n",
        "\n",
        "\n",
        "The maximum likelihood estimator $\\hat{\\theta}_{\\text{MLE}}$ is defined as the value of $\\theta$ that maximises the likelihood function:\n",
        "\n",
        "$$\n",
        "\\hat{\\theta}_{\\text{MLE}} = \\arg \\max_{\\theta} L(\\theta) = \\arg \\max_{\\theta} \\ell(\\theta).\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "6jQN-qgyrzXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In practice, we often optimise the log-likelihood function instead of the likelihood function, even though they are equivalent. This is because the log-likelihood is a sum rather than a product, making its derivatives easier to compute.\n",
        "\n",
        "MLE has many desirable properties in large samples (beyond the scope of this unit). In many scenarios, MLE and MoM result in the same estimator. However, unlike MoM, it always respects the constraints of the parameter space as we optimise for $\\theta$ over its parameter space. The main drawback is that computing $\\hat{\\theta}_{\\text{MLE}}$ can sometimes require solving nonlinear equations or numerical optimisation. Here, however, we will primarily focus on models for which the maximum likelihood estimator, $\\hat{\\theta}_{\\text{MLE}}$, can be derived analytically."
      ],
      "metadata": {
        "id": "DlwlZxXhGyEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **\"All models are wrong, but some are more useful\"**\n",
        "\n",
        "- **True model class:** The actual distribution generating the data (usually unknown, could be complex).  \n",
        "- **Approximation model class:** The set of models you choose to fit (e.g., Gaussian, linear regression).  \n",
        "- **Estimation:** Finds the best-fitting model **within the approximation class** according to some criteria (e.g., MoM, MLE, ⋯), but if the class does not contain the true model, there is still some error (model mis-specification), which could be large or small.\n"
      ],
      "metadata": {
        "id": "I1eSA3qLNZlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Workshop Questions**"
      ],
      "metadata": {
        "id": "sQsNEVzjt8J2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 1**\n",
        "\n",
        "A factory produces light bulbs. Each bulb has a probability $p$ of being defective. A quality inspector takes a sample of 10 bulbs at random from the production line, and records the number of defective bulbs. The number of defective bulbs in $m=10$ such independent samples (each sample consisting of $n = 10$ bulbs) is recorded as follows:\n",
        "\n",
        "$$\n",
        "\\{2, 1, 0, 3, 1, 2, 1, 0, 2, 1\\}\n",
        "$$\n",
        "\n",
        "Assume that the number of defective bulbs in each sample follows `Binomial(n,p)`.  \n",
        "\n",
        "- Use the MoM and MLE to estimate the probability $p$ of a defective bulb.\n",
        "- Comment on the results"
      ],
      "metadata": {
        "id": "WmM8CSFA3ZpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smpl_data = c(2,1,0,3,1,2,1,0,2,1)"
      ],
      "metadata": {
        "id": "hzu8zcE54my-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method of Moments**.\n",
        "  \n",
        "For $X \\sim \\text{Binomial}(n=10, p)$, the first theoretical moment is :\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "m_1 = \\mathbb{E}[X] = np.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "From the sample $x_1, \\dots, x_{m}$, we compute the corresponding sample moment:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\hat{m}_1 &= \\frac{\\sum_{i=1}^m x_i}{m}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Equating the first theoretical and sample moments gives us:\n",
        "\n",
        "$$\\hat{p} = \\frac{\\hat{m}_1}{n} = 0.13.$$"
      ],
      "metadata": {
        "id": "VxbfwGZT-ePz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smpl_data = c(2,1,0,3,1,2,1,0,2,1)\n",
        "mean(smpl_data)/10"
      ],
      "metadata": {
        "id": "DmCqL5Km-lFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maximum Likelihood Estimation**\n",
        "$$\n",
        "\\begin{align}\n",
        "L(p) &= \\prod_{i=1}^{m} \\binom{n}{x_i} p^{x_i}(1-p)^{n-x_i} \\\\\n",
        "\\implies \\ell(p) &= \\Big(\\sum_{i=1}^m x_i\\Big)\\log p + \\Big(nm - \\sum_{i=1}^m x_i\\Big)\\log(1-p). \\\\\n",
        "\\implies \\frac{d\\ell}{dp} &= \\frac{\\sum_{i=1}^m x_i}{p} - \\frac{nm - \\sum_{i=1}^m x_i}{1-p}.\n",
        "\\end{align}\n",
        "$$\n",
        "Setting the first derivative to 0:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{d\\ell}{dp} &= \\frac{\\sum_{i=1}^m x_i}{p} - \\frac{nm - \\sum_{i=1}^m x_i}{1-p} = 0 \\\\[6pt]\n",
        "\\implies \\frac{\\sum x_i}{p} &= \\frac{nm - \\sum x_i}{1-p} \\\\[6pt]\n",
        "\\implies (1-p)\\sum x_i &= p\\big(nm - \\sum x_i\\big) \\\\[6pt]\n",
        "\\implies \\sum x_i - p\\sum x_i &= pnm - p\\sum x_i \\\\[6pt]\n",
        "\\implies \\sum x_i &= p\\, nm \\\\[6pt]\n",
        "\\implies \\hat p &= \\frac{\\sum_{i=1}^m x_i}{nm}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^m x_i = 13 \\;\\;\\implies\\;\\; \\hat{p} = \\frac{13}{100} = 0.13.\n",
        "$$\n",
        "\n",
        "Technically, after finding the MLE by setting the first derivative to zero, we should check the second derivative at the estimated value to ensure it is negative, which confirms that the solution is indeed a maximum (not a minimum or a saddle point).\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{d^2 \\ell}{dp^2} &= -\\frac{\\sum_{i=1}^m x_i}{p^2} - \\frac{nm - \\sum_{i=1}^m x_i}{(1-p)^2}.\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "lBTvXDSJ-tyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smpl_data = c(2,1,0,3,1,2,1,0,2,1)\n",
        "n = 10\n",
        "m = length(smpl_data)\n",
        "\n",
        "# MLE estimate of p\n",
        "p_hat <- sum(smpl_data) / (n * m)\n",
        "p_hat\n",
        "\n",
        "# Second derivative of log-likelihood at MLE\n",
        "second_derivative = -sum(smpl_data)/p_hat^2 - (n*m - sum(smpl_data))/(1 - p_hat)^2\n",
        "second_derivative #-884 < 0"
      ],
      "metadata": {
        "id": "d8dBuiIU-z1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 2**\n",
        "\n",
        "The following code cell loads daily returns of `S&P 500` index from `03/01/2000` to `18/07/2025` from `s&p500.csv`, queried from Yahoo Finance."
      ],
      "metadata": {
        "id": "kqPj58PXt_yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp500_daily.returns = read.csv(\"datasets/s&p500.csv\")\n",
        "sp500_daily.returns %>% str()"
      ],
      "metadata": {
        "id": "r9K431O5AJW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Question 2.1**\n",
        "\n",
        "Assuming that daily returns $x_1,...,x_{6424}$ were generated from i.i.d. $\\mathcal{N}(\\mu, \\sigma^2)$, derive the MoM estimators for $\\mu$ and $\\sigma^2$. Comment on the results."
      ],
      "metadata": {
        "id": "i8ruOFICAO3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$.  \n",
        "The first two theoretical raw moments are:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "m_1 &= \\mathbb{E}[X] = \\mu, \\\\\n",
        "m_2 &= \\mathbb{E}[X^2] = \\mu^2 + \\sigma^2.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "From the sample $x_1, \\dots, x_n$, the corresponding sample moments are:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\hat{m}_1 &= \\frac{1}{n} \\sum_{i=1}^n x_i, \\\\\n",
        "\\hat{m}_2 &= \\frac{1}{n} \\sum_{i=1}^n x_i^2.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Equating the first theoretical and sample moments gives:\n",
        "\n",
        "$$\n",
        "\\hat{\\mu} = \\hat{m}_1 = \\bar{x}.\n",
        "$$\n",
        "\n",
        "Equating the second moments:\n",
        "\n",
        "$$\n",
        "\\hat{m}_2 = \\mu^2 + \\sigma^2.\n",
        "$$\n",
        "\n",
        "Substituting $\\mu = \\hat{m}_1$:\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}^2 = \\hat{m}_2 - \\hat{\\mu}^2\n",
        "= \\frac{1}{n}\\sum_{i=1}^n x_i^2 - \\left(\\frac{1}{n}\\sum_{i=1}^n x_i\\right)^2 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{n}.\n",
        "$$\n",
        "\n",
        "This is sample variance **without the (n-1) correction** (i.e., the usual sample variance). This is a biased estimator but asymptotically equivalent to $s^2$ as $\\frac{n-1}{n} \\rightarrow 1$ as $n \\rightarrow \\infty$.\n",
        "\n",
        "**FYI**: The biased estimator could be better:\n",
        "\n",
        "Despite being biased, this estimator might have a smaller variance and mean squared error (MSE) — provably so under the Gaussian assumption — and can therefore be more accurate when the sample size $n$ is small."
      ],
      "metadata": {
        "id": "uJh-_fNP-5l8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Question 2.2**\n",
        "\n",
        "Use R to compute the MoM estimates of $\\mu$ and $\\sigma^2$."
      ],
      "metadata": {
        "id": "0vgyuSJVA-Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = nrow(sp500_daily.returns)\n",
        "mean_est = mean(sp500_daily.returns$daily.returns)\n",
        "var_est = var(sp500_daily.returns$daily.returns)*(n-1)/n"
      ],
      "metadata": {
        "id": "a4dsi-ZwAALE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 2.3**\n",
        "\n",
        "What are the 0.0005 and 0.9995 empirical quantiles of daily returns, and what is the probability of observing a return at least as extreme as these quantile values under the MoM fitted distribution. Comment on the results."
      ],
      "metadata": {
        "id": "A4CrWl-FDAnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q0005 = quantile(sp500_daily.returns$daily.returns, probs = 0.0005)\n",
        "q9995 = quantile(sp500_daily.returns$daily.returns, probs = 0.9995)\n",
        "\n",
        "n = nrow(sp500_daily.returns)\n",
        "mean_est = mean(sp500_daily.returns$daily.returns)\n",
        "var_est = var(sp500_daily.returns$daily.returns)*(n-1)/n\n",
        "\n",
        "Pr_largerthanq9995 = pnorm(q9995, mean_est, sd = sqrt(var_est), lower.tail = F)\n",
        "Pr_smallerthanq0005= pnorm(q0005, mean_est, sd = sqrt(var_est), lower.tail = T)\n",
        "\n",
        "Pr_largerthanq9995\n",
        "Pr_smallerthanq0005"
      ],
      "metadata": {
        "id": "OBl11SRaEPf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The probability is of order $10^{-14}$ to $10^{-13}$, implying the fitted Gaussian model underestimates the tail probabilities."
      ],
      "metadata": {
        "id": "HPvi6ghm_HHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Question 2.4**\n",
        "\n"
      ],
      "metadata": {
        "id": "qD-9_ECNGDZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use qqplots to verify whether or not the Gaussian assumption is adequate."
      ],
      "metadata": {
        "id": "IAnk7mtIGOq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qqnorm(sp500_daily.returns$daily.returns)\n",
        "qqline(sp500_daily.returns$daily.returns)"
      ],
      "metadata": {
        "id": "dUN_WVtn30kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This suggests a clear violation of the Gaussian assumption."
      ],
      "metadata": {
        "id": "E71PU1Tp_QiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Question 2.5**\n",
        "\n",
        "Modify the following code cell to overlay the density of the MoM-fitted Gaussian distribution on the (normalised) histogram of daily returns. Comment on the results."
      ],
      "metadata": {
        "id": "EF1etNbzGpiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "set.seed(1)\n",
        "df =  data.frame(x = rnorm(1000))\n",
        "n = nrow(sp500_daily.returns)\n",
        "mean_est = mean(sp500_daily.returns$daily.returns)\n",
        "var_est = var(sp500_daily.returns$daily.returns)*(n-1)/n\n",
        "\n",
        "sp500_daily.returns %>%\n",
        "  ggplot(aes(x = daily.returns)) +\n",
        "  geom_histogram(aes(y = after_stat(density)), bins = 50,\n",
        "                 fill = \"lightgray\", color = \"white\") +\n",
        "  stat_function(fun = dnorm, #This evaluates pdf of Gaussian with the specified mean and sd and overlays a line curve on top of the existing one\n",
        "                args = list(mean = mean_est, sd = sqrt(var_est)),\n",
        "                color = \"red\", size = 1) +\n",
        "  labs(title = \"S&P500 Daily Returns with MoM Gaussian Fit\",\n",
        "       x = \"Daily return\", y = \"Density\") +\n",
        "  geom_rug()+\n",
        "  theme_minimal()\n"
      ],
      "metadata": {
        "id": "A9-Ql7V-7DRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Interval Estimation**"
      ],
      "metadata": {
        "id": "uRHV_m3s9X4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In many scenarios, we are interested in creating an interval that hopefully captures the parameter’s true value instead of obtaining a single point estimate:\n",
        "\n",
        "$$\\Big[T_1\\big(\\{x_1,...,x_n\\}\\big), T_2\\big(\\{x_1,...,x_n\\}\\big)\\Big].$$\n",
        "\n",
        "A common rule option $1−\\alpha$ confidence interval, which aims to find functions $T_1$ and $T_2$ such that:\n",
        "\n",
        "$$\\Pr\\Big(T_1\\big(\\{x_1,...,x_n\\}\\big) \\leq \\theta \\leq T_2\\big(\\{x_1,...,x_n\\}\\big)\\Big) = 1 - \\alpha.$$\n",
        "\n",
        "Conventionally, $\\alpha = 0.05$ (95% confidence interval). Other common options include $\\alpha = 0.01$ and $\\alpha = 0.1$.\n",
        "\n",
        "\n",
        "Note that, **pre-sample**, the upper and lower bounds of a confidence interval are random variables. This is why the previous equation is mathematically valid. The upper and lower bound estimates computed after observing the sample data are realisations of these random variables.\n",
        "\n",
        "Here, we mostly focus on confidence intervals for population mean and difference between two population means using asymptotic sampling distribution of the sample mean. Small-sample confidence interval (i.e., not relying on asymptotic properties) will be covered in the next workshops. Confidence intervals for more general estimators (like MLE) are beyond the scope of the unit."
      ],
      "metadata": {
        "id": "o41nwYQc9anw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Asymptotic Confidence Interval for Population Mean**\n",
        "\n",
        "Recall that, for i.i.d. sample of size $n$ drawn from any probability distribution with mean $\\mu$ and variance $\\sigma^2$, if $n$ is large enough,\n",
        "$$\n",
        "\\begin{align}\n",
        "&\\quad\\quad\\;\\,\\,\\displaystyle \\bar{x} \\approx \\mathcal{N}\\Big(\\mu, \\frac{\\sigma^2}{n}\\Big);\\\\\n",
        "&\\implies  \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}} \\approx \\mathcal{N}(0,1); \\\\\n",
        "&\\implies \\Pr\\Big(z_{\\alpha/2} \\leq \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}} \\leq z_{1-\\alpha/2}\\Big) \\approx 1 - \\alpha;\\\\\n",
        "&\\implies \\Pr\\Big(\\bar{x} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{x} + z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\Big) \\approx 1 - \\alpha\\\\\n",
        "&\\implies \\Pr\\Big(\\bar{x} - z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{x} + z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\Big) \\approx 1 - \\alpha.\n",
        "\\end{align}\n",
        "$$  \n",
        "\n",
        "Here, $z_{\\alpha/2}$ and $z_{1-\\alpha/2}$ are $\\alpha/2$ and $1-\\alpha/2$ quantiles of the standard Gaussian distribution $\\mathcal{N}(0,1)$. As the standard Gaussian PDF is symmetric around 0, $z_{\\alpha/2} = -z_{1-\\alpha/2}$.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XvvKrqv0L_jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Note that our notation is a bit different from the notation used in pre-lecture reading**. Here, $z_{1-\\alpha}$ is used to denote the $1-\\alpha$ quantile (lower-tail notation), while it is $z_{\\alpha}$ in the pre-lecture reading (upper-tail notation)."
      ],
      "metadata": {
        "id": "WGGaTHfCa8FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = readPNG(\"figures/z_95int.png\")\n",
        "grid.newpage()\n",
        "vp = viewport()\n",
        "pushViewport(vp)\n",
        "grid.raster(img)\n",
        "popViewport()"
      ],
      "metadata": {
        "id": "Z2Af8mffSNTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An asymptotically valid confidence interval for population mean can be obtained by choosing:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} - z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}};\\\\\n",
        "T_2\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} + z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Confidence intervals constructed following this rule are symmetric around sample means.\n",
        "\n",
        "A $95\\%$ symmetric confidence interval can be constructed by choosing:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} - z_{0.975}\\frac{\\sigma}{\\sqrt{n}};\\\\\n",
        "T_2\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} + z_{0.975}\\frac{\\sigma}{\\sqrt{n}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Of course, one can construct an annoying asymmetrical confidence interval like this:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} + z_{0.01}\\frac{\\sigma}{\\sqrt{n}};\\\\\n",
        "T_2\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} + z_{0.96}\\frac{\\sigma}{\\sqrt{n}};\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Or an annoying $61\\%$ symmetric confidence interval (we might be the first ones to use a $61\\%$ confidence interval):\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} + z_{0.195}\\frac{\\sigma}{\\sqrt{n}};\\\\\n",
        "T_2\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} + z_{0.805}\\frac{\\sigma}{\\sqrt{n}}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "If data are generated from i.i.d. Gaussians, these $1-\\alpha$ confidence intervals are **ALWAYS** valid regardless of the sample size (since the sample mean will be exactly normally distributed in this case).\n",
        "\n",
        "\n",
        "\n",
        "In practice, the standard deviation $\\sigma$ (or equivalently, the variance $\\sigma^2$) is usually unknown. However, we can replace $\\sigma$ by the sample standard deviation estimate $s$. This is a plug-in estimator.\n",
        "\n",
        "> A plug-in estimator is obtained by taking a formula that involves an unknown parameter and plugging in an estimate of that parameter (usually computed from data).\n",
        "\n",
        "Here, for instance, we can replace the unknown $\\sigma$ with $s = \\sqrt{s^2}$ (the unbiased/usual sample variance).\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} - z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}};\\\\\n",
        "T_2\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} + z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}}.\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "Ube_8GClWRWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Asymptotic Confidence Interval for Population Proportion**\n",
        "\n",
        "If $n$ is sufficiently large,\n",
        "\n",
        "$$\n",
        "\\displaystyle \\hat{p} \\approx \\mathcal{N}\\Big(p, \\frac{p(1-p)}{n}\\Big).\n",
        "$$  \n",
        "\n",
        "Therefore, we can obtain a similar asymptotic symmetric $(1-\\alpha)$ confidence interval for population proportion $p$ by choosing:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_1,...,x_n\\}\\big) &= \\hat{p} - z_{1-\\alpha/2}\\sqrt{\\frac{p(1-p}{n}};\\\\\n",
        "T_2\\big(\\{x_1,...,x_n\\}\\big) &= \\hat{p} + z_{1-\\alpha/2}\\sqrt{\\frac{p(1-p}{n}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z1AfZCBNAPuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, $p$ is unknown. However, we can replace $p$ with its estimator $\\hat{p}$:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_1,...,x_n\\}\\big) &= \\hat{p} - z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}};\\\\\n",
        "T_2\\big(\\{x_1,...,x_n\\}\\big) &= \\hat{p} + z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}.\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "P3d3jwBACGpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Finding Quantiles**\n",
        "\n",
        "You can find quantiles of the standard Gaussian distribution in a $Z$ statistical table."
      ],
      "metadata": {
        "id": "2NPiduMMaOkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generateZTable()"
      ],
      "metadata": {
        "id": "_6lAJGxtaS16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Examples**\n",
        "\n"
      ],
      "metadata": {
        "id": "ndOGR1X9TLzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Example 1**\n",
        "\n",
        "The following sample are generated from i.i.d. random variables with unknown mean $\\mu$ and variance $\\sigma^2 = 1$.\n",
        "\n",
        "$$\\{0.13, 0.68, 0.26, 0.97, 1.2, 0.74, 0.42, 0.2, 2.13, 0.06, 1.18, 0.25, 3.12, 4.16, 2.05, 0.21, 2.14,2,0.85,0.07,0.04,0.77,0.3,0.53,0.3,0.82,0.53,0.08,0.22,0.14,0.17,0.81,0.29,0.57,0.08,0.2,1.94,3.34,2.09,0.95,0.44\\}$$\n",
        "\n",
        "Compute a $90\\%$ symmetric confidence interval estimate for $\\mu$ using the asymptotic distribution of $\\bar{x}$ using\n",
        "- The true variance\n",
        "- The estimated variance\n",
        "\n",
        "Comment on the results."
      ],
      "metadata": {
        "id": "XL3qVbDcVUxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smpl_data = c(0.13, 0.68, 0.26, 0.97, 1.2, 0.74, 0.42, 0.2, 2.13, 0.06, 1.18, 0.25, 3.12, 4.16, 2.05, 0.21, 2.14,2,0.85,0.07,0.04,0.77,0.3,0.53,0.3,0.82,0.53,0.08,0.22,0.14,0.17,0.81,0.29,0.57,0.08,0.2,1.94,3.34,2.09,0.95,0.44)\n",
        "\n",
        "x_bar = mean(smpl_data)\n",
        "print(\"x_bar\")\n",
        "x_bar\n",
        "\n",
        "z_95 = qnorm(0.95)\n",
        "print(\"z_95\")\n",
        "z_95\n",
        "\n",
        "sigma = 1\n",
        "\n",
        "s = sd(smpl_data)\n",
        "print(\"s\")\n",
        "s\n",
        "\n",
        "print(\"n\")\n",
        "n = length(smpl_data)\n",
        "n\n",
        "\n",
        "#Known sigma\n",
        "print(\"Known sigma\")\n",
        "lower = x_bar - z_95*sigma/sqrt(n)\n",
        "upper = x_bar + z_95*sigma/sqrt(n)\n",
        "lower; upper\n",
        "\n",
        "#Plug-in\n",
        "print(\"Plug-in\")\n",
        "lower = x_bar - z_95*s/sqrt(n)\n",
        "upper = x_bar + z_95*s/sqrt(n)\n",
        "lower; upper\n"
      ],
      "metadata": {
        "id": "H37cWKTbUZf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confidence interval for population mean can be obtained by choosing:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} - z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}};\\\\\n",
        "T_2\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} + z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Here,\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\bar{x} &\\approx 0.9129;\\\\\n",
        "s &= 0.9965;\\\\\n",
        "\\sigma &= 1.0000;\\\\\n",
        "n &= 41;\\\\\n",
        "z_{0.95} &\\approx 1.6449.\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Given known $\\sigma^2$, at the 95% confidence level, a plausible range for $\\mu$ is:\n",
        "\n",
        "$$[0.6560, 1.1698].$$\n",
        "\n",
        "\n",
        "We could also replace $\\sigma$ with its estimator $s$:\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} - z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}};\\\\\n",
        "T_2\\big(\\{x_1,...,x_n\\}\\big) &= \\bar{x} + z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "Given unknown $\\sigma^2$, at the 95% confidence level, a plausible range for $\\mu$ is:\n",
        "\n",
        "$$[0.6570,1.1689].$$\n",
        "\n",
        "These confidence interval estimates are quite similar.\n",
        "\n"
      ],
      "metadata": {
        "id": "zNy9vz4XNEtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Example 2**\n",
        "\n",
        "A survey was conducted among 200 university students to see how many of them regularly drink coffee before lectures. Out of the 200 students, 128 said \"yes\".\n",
        "\n",
        "Assuming a suitable statistical model, construct a symmetric 95% confidence interval for the true population proportion $p$ using the asymptotic normal approximation."
      ],
      "metadata": {
        "id": "ozyDHMHpDZ71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"p_hat\")\n",
        "p_hat = 128/200\n",
        "p_hat\n",
        "\n",
        "z_975 = qnorm(0.975)\n",
        "print(\"z_975\")\n",
        "z_975\n",
        "\n",
        "n = 200\n",
        "\n",
        "print(\"Plug-in CI\")\n",
        "lower = p_hat - z_975*sqrt(p_hat*(1-p_hat)/n)\n",
        "upper = p_hat + z_975*sqrt(p_hat*(1-p_hat)/n)\n",
        "lower;upper\n"
      ],
      "metadata": {
        "id": "qaI0aPKGDbhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume that the responses of the 200 students, $x_1, \\dots, x_{200}$, take values 1 (\"yes\") or 0 (\"no\") and are i.i.d. `Bernoulli(p)` (Equivalently, the total number of \"yes\" responses follows a `Binomial(200, p)` distribution).\n",
        "\n",
        "A plug-in confidence interval for $p$ can be constructed by choosing:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_1,...,x_n\\}\\big) &= \\hat{p} - z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}};\\\\\n",
        "T_2\\big(\\{x_1,...,x_n\\}\\big) &= \\hat{p} + z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Here,\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\hat{p} &\\approx 0.64;\\\\\n",
        "n &= 200;\\\\\n",
        "z_{0.975} &\\approx 1.9600.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "At the 95% confidence level, a plausible range for $p$ is:\n",
        "\n",
        "$$[0.5734; 0.7065]$$\n",
        "\n"
      ],
      "metadata": {
        "id": "F29YRHqeRPFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Confidence Interval for The Difference Between Two Population Means**\n",
        "\n",
        "In addition to estimating population means, often we are interested in the difference between the population means $\\mu_1$ and $\\mu_2$ of two **independent** populations $f_1(x)$ and $f_2(x)$.\n",
        "\n",
        "It is reasonable (and correct) to assume that the best estimate of the difference between population means $\\mu_1 - \\mu_2$ is the difference between the sample means $\\bar{x}_1 - \\bar{x}_2$. The question is then, what is the sampling distribution of $\\bar{x}_1 - \\bar{x}_2$?\n",
        "\n",
        "Given $\\{x_{11},...,x_{1n_1}\\}$ and $\\{x_{21},...,x_{2n_2}\\}$ generated from i.i.d. probability distributions $f_1(x)$ and $f_2(x)$ with mean $\\mu_1$ and $\\mu_2$ and variance $\\sigma^2_1$ and $\\sigma^2_2$, respectively. If $n_1$ and $n_2$ are sufficiently large:\n",
        "\n",
        "$$\\bar{x}_1 - \\bar{x}_2 \\approx \\mathcal{N}\\Bigg(\\mu_1 - \\mu_2, \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\Bigg)$$\n",
        "\n",
        "\n",
        "If $f_1(x)$ and $f_2(x)$ are Gaussians, this is the exact sampling distribution of $\\bar{x}_1 - \\bar{x}_2$ regardless of the sample size."
      ],
      "metadata": {
        "id": "BMdNcUuqC4O0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A symmetric confidence interval for $\\mu_1 - \\mu_2$ can be obtained by choosing:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_{11},...,x_{1n_1}, x_{21},...,x_{2n_1}\\}\\big) &= (\\bar{x}_1 - \\bar{x}_2) - z_{1-\\alpha/2}\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}};\\\\\n",
        "T_2\\big(\\{x_{11},...,x_{1n_1}, x_{21},...,x_{2n_1}\\}\\big) &= (\\bar{x}_1 - \\bar{x}_2) + z_{1-\\alpha/2}\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "A plug-in symmetric confidence interval for $\\mu_1 - \\mu_2$ can be obtained by choosing:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_{11},...,x_{1n_1}, x_{21},...,x_{2n_1}\\}\\big) &= (\\bar{x}_1 - \\bar{x}_2) - z_{1-\\alpha/2}\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}};\\\\\n",
        "T_2\\big(\\{x_{11},...,x_{1n_1}, x_{21},...,x_{2n_1}\\}\\big) &= (\\bar{x}_1 - \\bar{x}_2) + z_{1-\\alpha/2}\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}},\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $s_1^2$ and $s_2^2$ are sample variances of samples 1 and 2, respectively.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uqeaZGOYDHk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Confidence Interval for The Difference Between Two Population Proportions**\n",
        "\n",
        "Given $\\{x_{11},...,x_{1n_1}\\}$ and $\\{x_{21},...,x_{2n_2}\\}$ generated from i.i.d. `Bernoulli(p1)` and `Bernoulli(p2)`, respectively. If $n_1$ and $n_2$ are sufficiently large:\n",
        "\n",
        "$$\\hat{p}_1 - \\hat{p}_2 \\approx \\mathcal{N}\\Bigg(p_1 - p_2, \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\Bigg)$$\n",
        "\n",
        "A symmetric confidence interval for $p_1 - p_2$ can be obtained by choosing:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_{11},...,x_{1n_1}, x_{21},...,x_{2n_1}\\}\\big) &= (\\hat{p}_1 - \\hat{p}_2) - z_{1-\\alpha/2}\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}};\\\\\n",
        "T_2\\big(\\{x_{11},...,x_{1n_1}, x_{21},...,x_{2n_1}\\}\\big) &= (\\hat{p}_1 - \\hat{p}_2) + z_{1-\\alpha/2}\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "A plug-in symmetric confidence interval for $p_1 - p_2$ can be obtained by choosing:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T_1\\big(\\{x_{11},...,x_{1n_1}, x_{21},...,x_{2n_1}\\}\\big) &= (\\hat{p}_1 - \\hat{p}_2) - z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}};\\\\\n",
        "T_2\\big(\\{x_{11},...,x_{1n_1}, x_{21},...,x_{2n_1}\\}\\big) &= (\\hat{p}_1 - \\hat{p}_2) + z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "26j6qQ6ADKCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example**\n",
        "\n",
        "A coffee shop wants to compare customer preferences between two types of coffee (A and B) when used in cappuccinos. To test this, they randomly assign customers to try cappuccinos made with either coffee A or coffee B.\n",
        "A total of 200 customers participate: 80 in group A and 120 in group B.\n",
        "- In group A, 61 customers say they like the cappuccino.\n",
        "- In group B, 97 customers say they like the cappuccino.\n",
        "\n",
        "\n",
        "Using a suitable statistical model, construct a 90% symmetric confidence interval for the difference in population proportions of customers who like coffee A versus coffee B based on the asymptotic normal approximation."
      ],
      "metadata": {
        "id": "k7QNBdhKDNa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nA = 80\n",
        "nB = 120\n",
        "pA_hat = 61/nA\n",
        "pB_hat = 97/nB\n",
        "print(\"pA_hat; pB_hat\")\n",
        "pA_hat; pB_hat\n",
        "\n",
        "z_95 = qnorm(0.95)\n",
        "print(\"z_95\")\n",
        "z_95\n",
        "\n",
        "print(\"Plug-in CI\")\n",
        "p_diff = pA_hat - pB_hat\n",
        "lower = p_diff - z_95*sqrt( pA_hat*(1-pA_hat)/nA  + pB_hat*(1-pB_hat)/nB)\n",
        "upper = p_diff + z_95*sqrt( pA_hat*(1-pA_hat)/nA  + pB_hat*(1-pB_hat)/nB)\n",
        "lower; upper"
      ],
      "metadata": {
        "id": "_M5bldiiDftI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A plug-in symmetric confidence interval for $p_1 - p_2$ can be obtained by choosing:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "T_1 &= (\\hat{p}_1 - \\hat{p}_2) - z_{1-\\alpha/2}\\sqrt{\\frac{\\hat p_1(1-\\hat p_1)}{n_1} + \\frac{\\hat p_2(1-\\hat p_2)}{n_2}},\\\\\n",
        "T_2 &= (\\hat{p}_1 - \\hat{p}_2) + z_{1-\\alpha/2}\\sqrt{\\frac{\\hat p_1(1-\\hat p_1)}{n_1} + \\frac{\\hat p_2(1-\\hat p_2)}{n_2}}.\n",
        "\\end{aligned}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "11WEQFGJf-3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here,\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\hat{p}_A &= 0.7625;\\\\\n",
        "\\hat{p}_B &\\approx 0.8083;\\\\\n",
        "n_A &= 80;\\\\\n",
        "n_B &= 120;\\\\\n",
        "z_{0.95} &\\approx 1.6449.\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "At the 95% confidence level, a plausible range for $p$ is:\n",
        "\n",
        "$$[-0.1439, 0.0522]$$"
      ],
      "metadata": {
        "id": "XEX7VUmpgTWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Making Sense of Confidence Intervals**\n",
        "\n",
        "Confidence intervals are often misunderstood. Let’s take a simple example:\n",
        "\n",
        "Suppose human heights are generated from a distribution with mean $\\mu$ and variance $\\sigma^2$. Given a sufficiently large sample, we can use the asymptotic approximation for the sampling distribution of the sample mean. Suppose we compute a 95% confidence interval for $\\mu$ to be [170, 190]. How should we interpret this?\n",
        "\n",
        "- **Incorrect interpretation #1**\n",
        "\n",
        "> The probability that the true mean $\\mu$ lies in [170, 190] is 95%.\n",
        "\n",
        "\n",
        "This is wrong because $\\mu$ is a fixed (but unknown) parameter, not a random variable. It is either in the interval or not, so its probability of lying inside the interval is 0 or 1.\n",
        "\n",
        "- **Incorrect interpretation #2**\n",
        "\n",
        "> We are 95% confident that [170, 190] contains the true mean $\\mu$.\n",
        "\n",
        "This is misleading because “95% confident” has no formal mathematical meaning. It suggests subjectivity rather than a precise probability statement.\n",
        "\n",
        "- **Correct interpretation**\n",
        "\n",
        "Before observing the data, the probability that the confidence interval contains $\\mu$ is 0.95. In other words, if we were to repeat the sampling process many times and construct a confidence interval from each sample, about 95% of those intervals would contain the true mean.\n",
        "\n",
        "A confidence interval estimate such as $[170,190]$ has no probabilistic interpretation once observed; however, it is still useful because it is constructed from a procedure that, with high probability (e.g., 95%), produces intervals covering the true parameter."
      ],
      "metadata": {
        "id": "J_9MT9Nva5Z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, suppose we draw samples of size 10 from an i.i.d. distribution $\\mathcal{N}(2, 1.5^2)$. In this setting, the true variance $\\sigma^2 = 1.5^2$ is known, but the true mean $\\mu = 2$ is unknown to us. In each iteration, we take a sample of 10 observations from $\\mathcal{N}(2, 1.5^2)$, construct a 95% symmetric confidence interval for $\\mu$, and then check whether the interval contains the true value."
      ],
      "metadata": {
        "id": "t7yNDmc_-E37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this a few times\n",
        "CI_visualiser()"
      ],
      "metadata": {
        "id": "fNExe_56caG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Workshop Questions**\n"
      ],
      "metadata": {
        "id": "JQbU2uwUDlPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 1**\n",
        "\n",
        "\n",
        "The following code cell loads daily returns of `S&P 500` index from `03/01/2000` to `18/07/2025` from `s&p500.csv`, queried from Yahoo Finance."
      ],
      "metadata": {
        "id": "HOVvjNN0O9_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp500_daily.returns = read.csv(\"datasets/s&p500.csv\")\n",
        "sp500_daily.returns %>% str()"
      ],
      "metadata": {
        "id": "7pXA57Y0PLSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Question 1.1**\n",
        "\n",
        "Assuming that daily returns $x_1,...,x_{6424}$ were generated from i.i.d. $\\mathcal{N}(\\mu, \\sigma^2)$, derive the MoM estimators for $\\mu$ and $\\sigma^2$. Comment on the results."
      ],
      "metadata": {
        "id": "tJWZqcTMPJNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$.  \n",
        "The first two theoretical raw moments are:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "m_1 &= \\mathbb{E}[X] = \\mu, \\\\\n",
        "m_2 &= \\mathbb{E}[X^2] = \\mu^2 + \\sigma^2.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "From the sample $x_1, \\dots, x_n$, the corresponding sample moments are:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\hat{m}_1 &= \\frac{1}{n} \\sum_{i=1}^n x_i, \\\\\n",
        "\\hat{m}_2 &= \\frac{1}{n} \\sum_{i=1}^n x_i^2.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Equating the first theoretical and sample moments gives:\n",
        "\n",
        "$$\n",
        "\\hat{\\mu} = \\hat{m}_1 = \\bar{x}.\n",
        "$$\n",
        "\n",
        "Equating the second moments:\n",
        "\n",
        "$$\n",
        "\\hat{m}_2 = \\mu^2 + \\sigma^2.\n",
        "$$\n",
        "\n",
        "Substituting $\\mu = \\hat{m}_1$:\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}^2 = \\hat{m}_2 - \\hat{\\mu}^2\n",
        "= \\frac{1}{n}\\sum_{i=1}^n x_i^2 - \\left(\\frac{1}{n}\\sum_{i=1}^n x_i\\right)^2 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{n}.\n",
        "$$\n",
        "\n",
        "This is sample variance **without the (n-1) correction** (i.e., the usual sample variance). This is a biased estimator but asymptotically equivalent to $s^2$ as $\\frac{n-1}{n} \\rightarrow 1$ as $n \\rightarrow \\infty$.\n",
        "\n",
        "**FYI**: The biased estimator could be better:\n",
        "\n",
        "Despite being biased, this estimator might have a smaller variance and mean squared error (MSE) — provably so under the Gaussian assumption — and can therefore be more accurate when the sample size $n$ is small.\n"
      ],
      "metadata": {
        "id": "fWB7nAGOPwNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Question 1.2**\n",
        "\n",
        "Use R to compute the MoM estimates of $\\mu$ and $\\sigma^2$."
      ],
      "metadata": {
        "id": "IQpbEOe5PRO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = nrow(sp500_daily.returns)\n",
        "mean_est = mean(sp500_daily.returns$daily.returns)\n",
        "var_est = var(sp500_daily.returns$daily.returns)*(n-1)/n"
      ],
      "metadata": {
        "id": "9DJ309b-PsDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Question 1.3**\n",
        "\n",
        "What are the 0.0005 and 0.9995 empirical quantiles of daily returns, and what is the probability of observing a return at least as extreme as these quantile values under the MoM fitted distribution. Comment on the results."
      ],
      "metadata": {
        "id": "bgVACUh9PQGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q0005 = quantile(sp500_daily.returns$daily.returns, probs = 0.0005)\n",
        "q9995 = quantile(sp500_daily.returns$daily.returns, probs = 0.9995)\n",
        "\n",
        "n = nrow(sp500_daily.returns)\n",
        "mean_est = mean(sp500_daily.returns$daily.returns)\n",
        "var_est = var(sp500_daily.returns$daily.returns)*(n-1)/n\n",
        "\n",
        "Pr_largerthanq9995 = pnorm(q9995, mean_est, sd = sqrt(var_est), lower.tail = F)\n",
        "Pr_smallerthanq0005= pnorm(q0005, mean_est, sd = sqrt(var_est), lower.tail = T)\n",
        "\n",
        "Pr_largerthanq9995\n",
        "Pr_smallerthanq0005"
      ],
      "metadata": {
        "id": "-IQ610ExPl1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "These probabilities is of order $10^{-14}$ to $10^{-13}$, implying the fitted Gaussian model underestimates the tail probabilities."
      ],
      "metadata": {
        "id": "2_TYaXhBPqSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Question 1.4**\n",
        "Use qqplots to verify whether or not the Gaussian assumption is adequate."
      ],
      "metadata": {
        "id": "PDYiZdHFPSwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qqnorm(sp500_daily.returns$daily.returns)\n",
        "qqline(sp500_daily.returns$daily.returns)"
      ],
      "metadata": {
        "id": "MsG0OvGZPh8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This suggests a clear violation of the Gaussian assumption."
      ],
      "metadata": {
        "id": "WL9xhFsTPkiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Question 1.5**\n",
        "\n",
        "Modify the following code cell to overlay the density of the MoM-fitted Gaussian distribution on the (normalised) histogram of daily returns. Comment on the results."
      ],
      "metadata": {
        "id": "0sLGsZGNPceY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = nrow(sp500_daily.returns)\n",
        "mean_est = mean(sp500_daily.returns$daily.returns)\n",
        "var_est = var(sp500_daily.returns$daily.returns)*(n-1)/n\n",
        "\n",
        "sp500_daily.returns %>%\n",
        "  ggplot(aes(x = daily.returns)) +\n",
        "  geom_histogram(aes(y = after_stat(density)), bins = 50,\n",
        "                 fill = \"lightgray\", color = \"white\") +\n",
        "  stat_function(fun = dnorm, #This evaluates pdf of Gaussian with the specified mean and sd and overlays a line curve on top of the existing one\n",
        "                args = list(mean = mean_est, sd = sqrt(var_est)),\n",
        "                color = \"red\", size = 1) +\n",
        "  labs(title = \"S&P500 Daily Returns with MoM Gaussian Fit\",\n",
        "       x = \"Daily return\", y = \"Density\") +\n",
        "  geom_rug()+\n",
        "  theme_minimal()\n"
      ],
      "metadata": {
        "id": "ium9RgY3PfCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following questions are based on the `episodes` dataset. While you are expected to use R to compute the answers, the underlying concepts are identical to those in pen-and-paper confidence interval calculations."
      ],
      "metadata": {
        "id": "AXbwtUsQO9FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = read.csv(\"./datasets/episodes.csv\")\n",
        "episodes %>% str()"
      ],
      "metadata": {
        "id": "DtDU8gejiYRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Note**: While these series have ended and you technically have the full “population” of some values (e.g., results of the Bechdel-Wallace test), we still ask you to compute a confidence interval for the unknown $p$. This may seem counter-intuitive, but you can think of it as follows:\n",
        "\n",
        "- The episode test results are treated as realisations from an unknown probability distribution $f$ (here, Bernoulli(p)).\n",
        "- Although the episodes are released, we are interested in the underlying process that generates these values. This includes not-yet-released episodes or hypothetical similar episodes — the confidence interval captures uncertainty about $p$ in this larger “generative model.”\n",
        "\n",
        "This provides a foundation for future techniques, such as hypothesis testing, where we may want to assess whether the difference between two proportions is statistically significant or simply due to random variation. Simply comparing the “complete” populations of Bechdel test results for two series is not sufficient; instead, we rely on a statistical model to quantify uncertainty."
      ],
      "metadata": {
        "id": "3HYUgu2Pm2na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hint**: After using `summarise()`, use `pull()` to extract each summary statistic. Below is a toy example."
      ],
      "metadata": {
        "id": "WA1svPQ63a8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set.seed(1)\n",
        "df = data.frame(X = rnorm(10^4))\n",
        "\n",
        "df %>%\n",
        "  summarise(meanX = mean(X),\n",
        "            varX = var(X)) -> summary_stats\n",
        "\n",
        "summary_stats %>% pull(meanX) -> meanX #Extracting `meanX`, which can be assigned to another variable\n",
        "meanX\n",
        "summary_stats %>% pull(varX)"
      ],
      "metadata": {
        "id": "VHgo3wPr3blF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Question 2**\n",
        "\n",
        "Write R code to find the 90% confidence interval for the mean of the `IMDB.Ranking` for Star Trek (The Original Series). Interpret the results for a non-statistician stakeholder."
      ],
      "metadata": {
        "id": "fYK_YPHHDvew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "episodes%>%\n",
        "  filter(Series == \"TOS\")%>%\n",
        "  summarise(xbar = mean(IMDB.Ranking),std_dev = sd(IMDB.Ranking), N = n()) -> params\n",
        "\n",
        "xbar = pull(params,xbar)\n",
        "s = pull(params,std_dev)\n",
        "n = pull(params,N)\n",
        "z_95 = qnorm(0.95)\n",
        "\n",
        "lower = xbar-z_95*s/sqrt(n)\n",
        "upper = xbar+z_95*s/sqrt(n)\n",
        "print(paste0(\"At 90% confidence level, a plausible range for the average IMDB ranking of TOS is [\", lower, \"; \", upper, \"]\"))"
      ],
      "metadata": {
        "id": "cHiqx48Fi7Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Question 3**\n",
        "\n",
        "Write R code to find the 95% confidence interval for the difference of the mean `IMDB.Ranking` for Star Trek (The Original Series) and Star Trek: The Next Generation. Interpret the results for a non-statistician stakeholder."
      ],
      "metadata": {
        "id": "hOCGRZywDwNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "episodes %>%\n",
        "  filter(Series ==\"TOS\")%>%\n",
        "  summarise(xbar = mean(IMDB.Ranking),\n",
        "            VAR = var(IMDB.Ranking),\n",
        "            N = n()) -> params.tos\n",
        "\n",
        "episodes %>%\n",
        "  filter(Series ==\"TNG\")%>%\n",
        "  summarise(xbar = mean(IMDB.Ranking),\n",
        "            VAR = var(IMDB.Ranking),\n",
        "            N = n()) -> params.tng\n",
        "\n",
        "xbar.tos = pull(params.tos,xbar)\n",
        "var.tos = pull(params.tos,VAR)\n",
        "n.tos = pull(params.tos,N)\n",
        "\n",
        "xbar.tng = pull(params.tng,xbar)\n",
        "var.tng = pull(params.tng,VAR)\n",
        "n.tng = pull(params.tng,N)\n",
        "\n",
        "z_975 = qnorm(0.975)\n",
        "\n",
        "lower=(xbar.tos-xbar.tng)-z_975*sqrt((var.tos/n.tos+var.tng/n.tng))\n",
        "upper=(xbar.tos-xbar.tng)+z_975*sqrt((var.tos/n.tos+var.tng/n.tng))\n",
        "\n",
        "print(paste0(\"At 95% confidence level, a plausible range for the average IMDB ranking difference between TOS and TNG is [\", lower, \"; \", upper, \"]\"))\n"
      ],
      "metadata": {
        "id": "7A4O59YBi8bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Question 4**\n",
        "\n",
        "Write R code to find the 99% confidence interval for the proportion of Star Trek: The Next Generation episodes that pass the Bechdel-Wallace Test. Interpret the results for a non-statistician stakeholder.\n",
        "\n",
        "**Note that this is not a statistical test!**\n",
        "\n",
        "> The Bechdel-Wallace Test assesses female representation in fiction by checking if a work features at least two named women who have a conversation about something other than a man.\n",
        "\n",
        "Source: [Bechdel test - Wikipedia](https://en.wikipedia.org/wiki/Bechdel_test)."
      ],
      "metadata": {
        "id": "aK4Tc5wSDzX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes %>% filter(Series ==\"TNG\")%>%\n",
        "  summarise(phat = mean(Bechdel.Wallace.Test), N = n()) -> params.tng\n",
        "\n",
        "p.tng = pull(params.tng,phat)\n",
        "n.tng = pull(params.tng,N)\n",
        "z_995 = qnorm(0.995)\n",
        "\n",
        "lower = p.tng-z_995*sqrt((p.tng*(1-p.tng)/n.tng))\n",
        "upper = p.tng+z_995*sqrt((p.tng*(1-p.tng)/n.tng))\n",
        "print(paste0(\"At 99% confidence level, a plausible range for the proportion of TNG episodes that pass the Bechdel-Wallace Test is [\", lower, \"; \", upper, \"]\"))\n"
      ],
      "metadata": {
        "id": "C_fAi6p4jc7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Question 5**\n",
        "\n",
        "Write R code to find the 95% confidence interval for the difference in proportions of episodes that pass the Bechdel-Wallace test between Star Trek: The Next Generation and Star Trek: Voyager. Interpret the results for a non-statistician stakeholder."
      ],
      "metadata": {
        "id": "rMH3qOx2D1KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes %>%\n",
        "  filter(Series ==\"TNG\")%>%\n",
        "  summarise(phat = mean(Bechdel.Wallace.Test), N = n()) -> params.tng\n",
        "\n",
        "p.tng = pull(params.tng,phat)\n",
        "n.tng = pull(params.tng,N)\n",
        "\n",
        "\n",
        "episodes %>%\n",
        "  filter(Series ==\"VOY\")%>%\n",
        "  summarise(phat = mean(Bechdel.Wallace.Test), N = n()) -> params.voy\n",
        "\n",
        "p.voy = pull(params.voy,phat)\n",
        "n.voy = pull(params.voy,N)\n",
        "\n",
        "z_975 = qnorm(0.975)\n",
        "\n",
        "lower = (p.tng-p.voy)-z_975*sqrt((p.tng*(1-p.tng)/n.tng)+(p.voy*(1-p.voy)/n.voy))\n",
        "upper = (p.tng-p.voy)+z_975*sqrt((p.tng*(1-p.tng)/n.tng)+(p.voy*(1-p.voy)/n.voy))\n",
        "print(paste0(\"At 95% confidence level, a plausible range for the difference in proportions of episodes that pass the Bechdel-Wallace test between TNG and VOY is [\", lower, \"; \", upper, \"]\"))"
      ],
      "metadata": {
        "id": "FOdWqV2Yjo9H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}