{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Week 8: Large Sample Hypothesis Testing**\n",
        "\n",
        "```\n",
        ".------------------------------------.\n",
        "|   __  ____  ______  _  ___ _____   |\n",
        "|  |  \\/  \\ \\/ / __ )/ |/ _ \\___  |  |\n",
        "|  | |\\/| |\\  /|  _ \\| | | | | / /   |\n",
        "|  | |  | |/  \\| |_) | | |_| |/ /    |\n",
        "|  |_|  |_/_/\\_\\____/|_|\\___//_/     |\n",
        "'------------------------------------'\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "ZtpJJ8BSHOaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the following examples, we will explore the concepts of (large-sample) hypothesis testing (LSHT) and examine their practical implications.\n"
      ],
      "metadata": {
        "id": "nJN_EjsDJ5My"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-Configurating the Notebook**"
      ],
      "metadata": {
        "id": "qJMHamzBgYY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Switching to the R Kernel on Colab**\n",
        "\n",
        "By default, Google Colab uses Python as its programming language. To use R instead, you’ll need to manually switch the kernel by going to **Runtime > Change runtime type**, and selecting R as the kernel. This allows you to run R code in the Colab environment.\n",
        "\n",
        "However, our notebook is already configured to use R by default. Unless something goes wrong, you shouldn’t need to manually change runtime type."
      ],
      "metadata": {
        "id": "RYNbU2mDgjJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing Required Packages**\n",
        "**Run the following lines of code**:"
      ],
      "metadata": {
        "id": "Sh6s1ChmglUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do not modify\n",
        "\n",
        "setwd(\"/content\")\n",
        "\n",
        "# Remove `MXB107-Notebooks` if exists,\n",
        "if (dir.exists(\"MXB107-Notebooks\")) {\n",
        "  system(\"rm -rf MXB107-Notebooks\")\n",
        "}\n",
        "\n",
        "# Fork the repository\n",
        "system(\"git clone https://github.com/edelweiss611428/MXB107-Notebooks.git\")\n",
        "\n",
        "# Change working directory to \"MXB107-Notebooks\"\n",
        "setwd(\"MXB107-Notebooks\")\n",
        "\n",
        "#\n",
        "invisible(source(\"R/preConfigurated.R\"))"
      ],
      "metadata": {
        "id": "hXjYwRwxgmqZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do not modify the following**"
      ],
      "metadata": {
        "id": "0xU1keysgotb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if (!require(\"testthat\")) install.packages(\"testthat\"); library(\"testthat\")\n",
        "\n",
        "test_that(\"Test if all packages have been loaded\", {\n",
        "\n",
        "  expect_true(all(c(\"ggplot2\", \"tidyr\", \"dplyr\", \"stringr\", \"magrittr\", \"knitr\") %in% loadedNamespaces()))\n",
        "\n",
        "})"
      ],
      "metadata": {
        "id": "VkOd7qZXgqr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd71ebe7-ed27-4b54-dcae-f12186c644c8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mTest passed\u001b[39m 🌈\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reference Tables for LSHT for Sample Means**"
      ],
      "metadata": {
        "id": "1WJSiLcxqAjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Scenario | Parameter | Null Hypothesis | Test Statistic (z) |\n",
        "|----------|-----------|----------------|----------------|\n",
        "| One-sample mean | $\\mu$ | $\\mu = \\mu_0$ | $\\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}$ |\n",
        "| One-sample proportion | $p$ | $p = p_0$ | $\\frac{\\hat{p}-p_0}{\\sqrt{p_0(1-p_0)/n}}$ |\n",
        "| Two-sample mean | $\\mu_1 - \\mu_2$ | $\\mu_1  = \\mu_2$ | $\\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}}$ |\n",
        "| Two-sample proportion | $p_1 - p_2$ | $p_1  = p_2$ | $\\frac{\\hat{p}_1-\\hat{p}_2}{\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}}$ |\n",
        "\n",
        "\n",
        "Under the null hypothesis, all the test statistics $z$ in these tables are approximately distributed as a standard Gaussian $N(0,1)$ (for large enough sample sizes). If $\\sigma$ is unknown, it can be replaced with the sample standard deviation.\n",
        "\n",
        "Any substantial deviation from the null hypothesis will tend to produce $z$ values that are unlikely under this standard Gaussian distribution, which is why extreme values of $z$ provide evidence against $H_0$. Even though any deviation from $H_0$ can provide evidence against it, the choice between a one-sided and a two-sided test depends on our research goal and the direction of interest.\n",
        "\n",
        "| Test Type | Alternative Hypothesis | Rejection Region |\n",
        "|-----------|----------------------|----------------|\n",
        "| One-sided (right) | $H_1: \\theta > \\theta_0$ | Reject $H_0$ if $z > z_{1-\\alpha}$ |\n",
        "| One-sided (left) | $H_1: \\theta < \\theta_0$ | Reject $H_0$ if $z < z_{\\alpha}$ |\n",
        "| Two-sided | $H_1: \\theta \\neq \\theta_0$ | Reject $H_0$ if $|z| > z_{1-\\alpha/2}$ |\n",
        "\n",
        "\n",
        "If we specifically care about deviations in one direction — for example, testing whether the average battery life is less than 8 hours — a one-sided test is appropriate. Allocating all of the Type I error $\\alpha$ to that direction increases the test’s ability to detect deviations that matter in practice.\n",
        "\n",
        "On the other hand, if deviations in either direction are meaningful — for instance, testing whether the average rating of a show differs from 7.7, whether higher or lower — a two-sided test is necessary. Splitting $\\alpha$ between both tails ensures we properly account for evidence against $H_0$ in either direction.\n",
        "\n",
        "There are other scenarios (e.g., both the null and the alternative hypotheses are intervals). However, they are out of the scope of this unit."
      ],
      "metadata": {
        "id": "KkwOCy-Xo3Xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Making Sense of Hypothesis Testing**\n"
      ],
      "metadata": {
        "id": "EoNyOvXKPpp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smpl_data = c(7.2, 8.53, 8.07, 7.99, 7.79, 7.77, 8.9, 7.64, 7.35, 8.45, 9.14, 7.93, 7.35, 7.52, 7.41, 8.27,\n",
        "7.55, 7.5, 8.53, 8.37, 8.17, 8.15, 8.02, 7.63,7.64, 8.83, 8.17, 7.41, 7.7, 8.21)"
      ],
      "metadata": {
        "id": "UXUcHNuKVw0p"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Demonstrating Example**\n",
        "\n",
        "A phone company advertises that the average battery life of their phones (when continuously watching videos), denoted as $\\mu$, is 8 hours.\n",
        "\n",
        "To verify this claim, an independent random sample of 30 phones was tested. Battery life is assumed to follow a normal distribution, and the population standard deviation is known to be 1 hour.\n",
        "\n",
        "**Hint**:\n",
        "- Use the asymptotic properties of the sample mean\n",
        "- Replace the unknown standard deviation $\\sigma$ with its estimate"
      ],
      "metadata": {
        "id": "jAkk973JdkZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write down the asymptotic sampling distribution of sample mean.**"
      ],
      "metadata": {
        "id": "YaOr7MjSZm9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given i.i.d. $x_1, \\ldots, x_n \\sim \\mathcal{N}(\\mu, \\sigma^2)$, we have:\n",
        "\n",
        "$$\\bar{x} \\sim \\mathcal{N}\\Big(\\mu,\\frac{\\sigma^2}{30}\\Big)$$\n",
        "\n",
        "This is the exact sampling distribution as $x_1, \\ldots, x_n$ are i.i.d. Gaussian random variables."
      ],
      "metadata": {
        "id": "Nfvp1kNaaCjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write down the null and alternative hypotheses for testing whether the company’s claim is correct.**"
      ],
      "metadata": {
        "id": "o7EUD2CTW9n-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Since we are testing whether or not there is evidence *against* the company’s claim that the average battery life is 8 hours, the alternative hypothesis should challenge this claim.  \n",
        "\n",
        "Because we are not concerned if the battery lasts longer than 8 hours (that would be favorable to consumers), we only test if it is **less** than 8 hours.  \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "H_0: \\mu &= 8 \\\\\n",
        "H_1: \\mu &< 8\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "This is a **left-tailed test** (or often simply referred to as one-tailed or one-sided test) of the mean."
      ],
      "metadata": {
        "id": "c-VZC_P0a11X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approximate the sampling distribution of the sample mean under the null hypothesis.**"
      ],
      "metadata": {
        "id": "HaeJCeUTbjF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var(smpl_data)"
      ],
      "metadata": {
        "id": "axv1EtgzHhar",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2e7e7584-abbc-465f-f37f-a0337e2b9697"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.252849310344828"
            ],
            "text/markdown": "0.252849310344828",
            "text/latex": "0.252849310344828",
            "text/plain": [
              "[1] 0.2528493"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\bar{x} \\sim \\mathcal{N}\\Big(7.9,\\frac{0.253}{30}\\Big)$$"
      ],
      "metadata": {
        "id": "gkxjqyJ3Hga6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the z-test statistic for testing the null hypothesis and derive the rejection region.**"
      ],
      "metadata": {
        "id": "edBS1yg8XT1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$$\n",
        "z = \\frac{\\bar{x} - \\mu_0}{\\sigma_{\\bar{x}}} \\approx \\frac{\\bar{x} - 8}{\\sqrt{\\frac{0.253}{30}}}\n",
        "$$\n",
        "\n",
        "\n",
        "At $\\alpha = 0.05$, we reject the null hypothesis if $z < z_{0.05} = -1.645$. Thus, the rejection region is $(-\\infty, -1.645)$."
      ],
      "metadata": {
        "id": "bbuSlo1mIUQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is this approach valid?**\n",
        "\n",
        "Under the null hypothesis (i.e., if $H_0$ is true), the test statistic follows (approximately) a standard Gaussian distribution:  \n",
        "\n",
        "$$\n",
        "z \\mid H_0 \\sim \\mathcal{N}(0,1)\n",
        "$$  \n",
        "\n",
        "Here, the probability of observing $z < -1.645$ under $H_0$ is 0.05, which is relatively unlikely. If we observe a test statistic less than -1.645, this provides evidence **against** the null hypothesis that $\\mu = 8$.  \n",
        "\n",
        "For example, if the true population mean is substantially smaller than 8, the sample mean is likely to be smaller, resulting in a more negative test statistic $z$.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QojWvOg0gLjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Given the sample data, compute the test statistic and state the Neyman-Pearson decision.**"
      ],
      "metadata": {
        "id": "Jt709RJoXzud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xbar = mean(smpl_data)\n",
        "s = sd(smpl_data)\n",
        "n = 30\n",
        "z = (xbar-8)/(s/sqrt(n))\n",
        "z"
      ],
      "metadata": {
        "id": "tHg0vZV-YCFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "494386d6-7c3a-48ea-d3be-891f16f51bfa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "-0.294098970877915"
            ],
            "text/markdown": "-0.294098970877915",
            "text/latex": "-0.294098970877915",
            "text/plain": [
              "[1] -0.294099"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As $z ≈ -0.294 ∉ (-∞, -1.645)$, there is no evidence against the null hypothesis that $\\mu = 8$. There is insufficient evidence to reject the null hypothesis."
      ],
      "metadata": {
        "id": "FfRpraVLRqve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Intuitions of Hypothesis Testing**"
      ],
      "metadata": {
        "id": "RGC2U6nLdvf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hypothesis Testing Looks For Evidence**"
      ],
      "metadata": {
        "id": "tCxq00UZbByj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis testing is based on the philosophy that if an event is unlikely under scenario A, but we still observe it in reality, this serves as evidence against scenario A (calling into question the validity or existence of A).\n",
        "\n",
        "By convention, the **null hypothesis** is set to represent the idea that \"nothing special is happening,\" while the **alternative hypothesis** is the one that *challenges* this assumption.  \n",
        "\n",
        "For example:  \n",
        "- If you want to test whether the average battery life is less than 8 hours, the null hypothesis would be:  \n",
        "  $$\n",
        "  H_0: \\mu = 8\n",
        "  $$  \n",
        "  This is the \"nothing special\" scenario.  \n",
        "\n",
        "- If you suspect someone might have malicious intent, the null hypothesis would be:  \n",
        "  $$\n",
        "  H_0: \\text{No malicious intent}\n",
        "  $$\n",
        "  The alternative would be:  \n",
        "  $$\n",
        "  H_1: \\text{Malicious intent}\n",
        "  $$  \n",
        "\n",
        "Of course, if you start observing lots of *suspicious* actions, those observations serve as **evidence against the null**, which may lead you to favour the alternative.\n"
      ],
      "metadata": {
        "id": "vfnHWcAkbAHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Hypothesis Testing Does Not Prove Truth**\n",
        "\n",
        "Hypothesis testing cannot establish whether a hypothesis is true or false—it only assesses whether the data provide sufficient evidence to reject the null hypothesis.\n",
        "\n",
        "Even if we reject the null hypothesis, this does **not** mean that $H_0$ is false. This is because we may still commit a **Type I error**, which occurs when we reject the null hypothesis even though it is actually true.  \n",
        "\n",
        "Therefore, we should **never say**:  \n",
        "- \"The null hypothesis is wrong.\"  \n",
        "- \"The alternative hypothesis is correct.\"\n",
        "- \"We accept the alternative hypothesis.\"\n",
        "\n",
        "Instead, say:\n",
        "\n",
        "- \"There is evidence against the null hypothesis.\"\n",
        "- \"We reject the null hypothesis in favour of the alternative.\"\n",
        "\n",
        "The good news is that the probability of a Type I error is something we can control.  Most conventional hypothesis testing procedures (such as Neyman-Pearson or Fisher’s p-value approach) are based on pre-specifying a Type I error probability, often denoted by $\\alpha$. A common choice is $\\alpha = 0.05$, which serves as the threshold for deciding whether the observed data provide sufficient evidence against $H_0$.  \n",
        "\n",
        "Back to this example:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "H_0: &\\text{ No malicious intent} \\\\\n",
        "H_1: &\\text{ Malicious intent}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "What if we observe no suspicious actions? Does that mean $H_0$ is `true`? Not necessarily — they may simply be waiting for an opportunity. In hypothesis testing, we also have **Type 2 error** - failing to reject the null hypothesis when it's actually false. As a result, failing to reject the null does not imply:\n",
        "\n",
        "- \"The null hypothesis is correct.\"\n",
        "- \"We accept the null hypothesis.\"\n",
        "\n",
        "Instead, we should conclude that\n",
        "- \"There is no (or insufficient) evidence against the null hypothesis.\"\n",
        "\n",
        "Unfortunately, there is an inherent trade-off between Type I and Type II errors. The Neyman–Pearson lemma shows how to construct the most powerful test for a given size (i.e., a fixed Type I error rate). This test minimises the Type II error among all tests with that Type I error. However, for any fixed Type I error rate, you cannot reduce the Type II error further. In practice, you first choose the Type I error rate you are willing to tolerate, and then apply Neyman–Pearson to obtain a test that achieves the best possible power against a given alternative.\n",
        "\n",
        "How to actually construct such tests is beyond the scope of this unit. Instead, we will only state a corollary of the Neyman–Pearson lemma, which shows how to determine the rejection region in the simple case of testing hypotheses about the sample means."
      ],
      "metadata": {
        "id": "sbXBHPhRbNtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Only Meaningful Deviation Matters**\n",
        "\n",
        "We do hypothesis testing not to determine whether $H_0$ is true, but rather to assess whether the data provide evidence of a **meaningful** deviation from the null hypothesis — usually the “nothing is happening” scenario that we care about.\n",
        "\n",
        "\n",
        "For example, suppose the true mean battery life is 7.999 hours instead of 8. Such a tiny difference is practically indistinguishable, so it does not matter. What matters is whether the observed data show a meaningful departure from the claimed value of 8 hours. In this case, it is very likely that we will fail to reject the null hypothesis $\\mu_0 = 8$, because the deviation is too small to detect. The Type II error rate will be high. Is that a problem? Not at all. Even if we fail to reject $H_0$ in most scenarios, this still indicates there is no evidence against the hypothesis that the mean battery life is 8, which is essentially very close to the truth.\n"
      ],
      "metadata": {
        "id": "FvW1syaFbY5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **The Danger of Post-hoc Hypotheses**\n",
        "\n",
        "It is very bad practice to adjust the hypothesis after looking at the data.\n",
        "\n",
        "Hypothesis testing assumes that the null and alternative hypotheses are specified before collecting or examining the data.\n",
        "Changing your hypothesis after observing the data (sometimes called “data snooping” or “p-hacking”) inflates the Type I error rate and makes your conclusions unreliable.\n",
        "Connection to the battery-life example:\n",
        "Suppose you originally want to test\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "H_0: \\mu &= 8 \\\\\n",
        "H_1: \\mu &< 8\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "If you peek at the data and see a mean around 7.95 hours, and then decide to only test a smaller deviation (say $\\mu < 7.9$) to get “nicer” results, you are **post-hoc adjusting the hypothesis**.  This biases the test: your Type I error is no longer controlled.\n",
        "\n",
        "The correct approach: decide in advance what deviation you want to detect (e.g., battery life shorter than 8 hours) and stick with it, regardless of what the observed sample mean turns out to be. If you want to change the hypotheses, you need to collect new data."
      ],
      "metadata": {
        "id": "Hi_IIwfdegJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Another issue, called multiple testing, can lead to invalid inferences if not properly addressed. This topic will be covered in the next lecture.**"
      ],
      "metadata": {
        "id": "kiclBtSbxxTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Workshop Questions**\n",
        "Through out this section, we assume a Type 1 error rate of 0.05.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u7rOC3o-fyix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 1**\n",
        "\n",
        "The following questions are based on the `episodes` dataset. While you are expected to use R to compute the answers, the underlying concepts are identical to those in pen-and-paper hypothesis testing calculations."
      ],
      "metadata": {
        "id": "gMPkgJnVhLdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = read.csv(\"./datasets/episodes.csv\")\n",
        "episodes %>% str()"
      ],
      "metadata": {
        "id": "lTezwUg3hPf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c21dde-6879-469e-8218-fa2563340db2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'data.frame':\t704 obs. of  57 variables:\n",
            " $ Series                        : chr  \"TOS\" \"TOS\" \"TOS\" \"TOS\" ...\n",
            " $ Series.Name                   : chr  \"The Original Series\" \"The Original Series\" \"The Original Series\" \"The Original Series\" ...\n",
            " $ Season                        : int  1 1 1 1 1 1 1 1 1 1 ...\n",
            " $ Episode                       : int  1 2 3 4 5 6 7 8 9 10 ...\n",
            " $ IMDB.Ranking                  : num  7.3 7.2 7.8 8 7.8 6.9 7.6 7.1 7.5 8.2 ...\n",
            " $ Title                         : chr  \"The Man Trap\" \"Charlie X\" \"Where No Man Has Gone Before\" \"The Naked Time\" ...\n",
            " $ Star.date                     : chr  \"1513.1\" \"1533.6\" \"1312.4\" \"1704.2\" ...\n",
            " $ Air.date                      : chr  \"8/9/66\" \"15/9/66\" \"22/9/66\" \"29/9/66\" ...\n",
            " $ Bechdel.Wallace.Test          : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Director                      : chr  \"Marc Daniels\" \"Lawrence Dobkin\" \"James Goldstone\" \"Marc Daniels\" ...\n",
            " $ Writer.1                      : chr  \"George Clayton Johnson\" \"Gene Rodenberry\" \"Samuel A Peeples\" \"John D. F. Black\" ...\n",
            " $ Writer.2                      : chr  NA \"D. C. Fontana\" NA NA ...\n",
            " $ Writer.3                      : chr  NA NA NA NA ...\n",
            " $ Writer.4                      : chr  NA NA NA NA ...\n",
            " $ Writer.5                      : chr  NA NA NA NA ...\n",
            " $ Writer.6                      : chr  NA NA NA NA ...\n",
            " $ Female.Director               : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Writer.1               : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Writer.2               : logi  FALSE TRUE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Writer.3               : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Writer.4               : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Writer.5               : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Writer.6               : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Executive.Producer.1          : chr  NA NA NA NA ...\n",
            " $ Executive.Producer.2          : chr  NA NA NA NA ...\n",
            " $ Executive.Producer.3          : chr  NA NA NA NA ...\n",
            " $ Co.Executive.Producer.1       : chr  NA NA NA NA ...\n",
            " $ Co.Executive.Producer.2       : chr  NA NA NA NA ...\n",
            " $ Co.Executive.Producer.3       : chr  NA NA NA NA ...\n",
            " $ Producer.1                    : chr  \"Gene Rodenberry\" \"Gene Rodenberry\" \"Gene Rodenberry\" \"Gene Rodenberry\" ...\n",
            " $ Producer.2                    : chr  NA NA NA NA ...\n",
            " $ Producer.3                    : chr  NA NA NA NA ...\n",
            " $ Producer.4                    : chr  NA NA NA NA ...\n",
            " $ Co.Producer.1                 : chr  NA NA NA NA ...\n",
            " $ Co.Producer.2                 : chr  NA NA NA NA ...\n",
            " $ Co.Producer.3                 : chr  NA NA NA NA ...\n",
            " $ Co.Producer.4                 : chr  NA NA NA NA ...\n",
            " $ Co.Producer.5                 : chr  NA NA NA NA ...\n",
            " $ Associate.Producer.1          : chr  \"John D. F. Black\" \"John D. F. Black\" \"Robert H. Justman\" \"John D. F. Black\" ...\n",
            " $ Associate.Producer.2          : chr  \"Robert H. Justman\" \"Robert H. Justman\" NA \"Robert H. Justman\" ...\n",
            " $ Supervising.Producer.1        : chr  NA NA NA NA ...\n",
            " $ Supervising.Producer.2        : chr  NA NA NA NA ...\n",
            " $ Supervising.Producer.3        : chr  NA NA NA NA ...\n",
            " $ Co.Supervising.Producer.1     : chr  NA NA NA NA ...\n",
            " $ Co.Supervising.Producer.2     : chr  NA NA NA NA ...\n",
            " $ Line.Producer                 : chr  NA NA NA NA ...\n",
            " $ Coordinating.Producer         : chr  NA NA NA NA ...\n",
            " $ Consulting.Producer.1         : chr  NA NA NA NA ...\n",
            " $ Consulting.Producer.2         : chr  NA NA NA NA ...\n",
            " $ Female.Executive.Producer     : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Co.Executive.Producer  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Producer               : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Co.Producer            : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Associate.Producer     : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Supervising.Producer   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Co.Supervising.Producer: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
            " $ Female.Line.Producer          : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 1.1**\n",
        "\n",
        "Is there evidence that the average IMDB rating of Star Trek: The Original Series episodes is lower than 7.7? Interpret the results for a non-statistician stakeholder."
      ],
      "metadata": {
        "id": "r-EopFOFf19C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H0: mu = 7.7\n",
        "#H1: mu < 7.7\n",
        "\n",
        "episodes %>%\n",
        "  filter(Series == \"TOS\") %>%\n",
        "  summarise(avgIMDB = mean(IMDB.Ranking),\n",
        "            sdIMDB = sd(IMDB.Ranking),\n",
        "            n = n(),\n",
        "            mu_H0 = 7.7,\n",
        "            z_05 = qnorm(0.05)) %>%\n",
        "  mutate(z_obs = (avgIMDB - mu_H0)/(sdIMDB/sqrt(n)))\n",
        "\n",
        "#As z_obs < z_05, at 95% significance level, there is evidence against the null hypothesis that the average IMDB ranking of TOS is 7.7.\n",
        "# We reject the null hypothesis in favour of the alternative one that the average rating of TOS is lower than 7.7\n"
      ],
      "metadata": {
        "id": "WtVRLhe6hhG0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "7e579e13-a502-411a-e209-da9f3f088ee7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 1 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>avgIMDB</th><th scope=col>sdIMDB</th><th scope=col>n</th><th scope=col>mu_H0</th><th scope=col>z_05</th><th scope=col>z_obs</th></tr>\n",
              "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td>7.53625</td><td>0.8038269</td><td>80</td><td>7.7</td><td>-1.644854</td><td>-1.822065</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 1 × 6\n\n| avgIMDB &lt;dbl&gt; | sdIMDB &lt;dbl&gt; | n &lt;int&gt; | mu_H0 &lt;dbl&gt; | z_05 &lt;dbl&gt; | z_obs &lt;dbl&gt; |\n|---|---|---|---|---|---|\n| 7.53625 | 0.8038269 | 80 | 7.7 | -1.644854 | -1.822065 |\n\n",
            "text/latex": "A data.frame: 1 × 6\n\\begin{tabular}{llllll}\n avgIMDB & sdIMDB & n & mu\\_H0 & z\\_05 & z\\_obs\\\\\n <dbl> & <dbl> & <int> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t 7.53625 & 0.8038269 & 80 & 7.7 & -1.644854 & -1.822065\\\\\n\\end{tabular}\n",
            "text/plain": [
              "  avgIMDB sdIMDB    n  mu_H0 z_05      z_obs    \n",
              "1 7.53625 0.8038269 80 7.7   -1.644854 -1.822065"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 1.2**\n",
        "\n",
        "\n",
        "Is there evidence that the mean IMDB rating of episodes of Star Trek: The Original Series differs from the mean rating of Star Trek: The Next Generation? Interpret the results for a non-statistician stakeholder."
      ],
      "metadata": {
        "id": "2PtbU1jrhUZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H0: mu_TOS - mu_TNG = 0\n",
        "#H1: mu_TOS - mu_TNG != 0\n",
        "\n",
        "episodes %>%\n",
        "  filter(Series %in%  c(\"TOS\", \"TNG\")) %>%\n",
        "  group_by(Series) %>%\n",
        "  summarise(avgIMDB = mean(IMDB.Ranking),\n",
        "            varIMDB = var(IMDB.Ranking),\n",
        "            n = n()) -> summaryStats\n",
        "\n",
        "TNG_stats = summaryStats %>% filter(Series == \"TNG\")\n",
        "TOS_stats = summaryStats %>% filter(Series == \"TOS\")\n",
        "\n",
        "z_obs = (TOS_stats$avgIMDB - TNG_stats$avgIMDB)/sqrt(TOS_stats$varIMDB/TOS_stats$n + TNG_stats$varIMDB/TNG_stats$n)\n",
        "z_975 = qnorm(0.975)\n",
        "\n",
        "abs(z_obs) > z_975\n",
        "\n",
        "#As abc(z_obs) < z_975, at 95% significance level, there is no evidence against the null hypothesis that\n",
        "#the average ranking of TOS is equal to that of TNG. Do not reject the null hypothesis."
      ],
      "metadata": {
        "id": "qTY4XikJhiC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d22f5513-9d8e-4843-b0a4-ab1d1a38d313"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "FALSE"
            ],
            "text/markdown": "FALSE",
            "text/latex": "FALSE",
            "text/plain": [
              "[1] FALSE"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 1.3**\n",
        "\n",
        "Is there evidence that the proportion of Star Trek: The Next Generation episodes that pass the Bechdel-Wallace Test is different from 0.4? Interpret the results for a non-statistician stakeholder."
      ],
      "metadata": {
        "id": "AWwDxhuKhv44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Note**: While these series have ended and you technically have the full “population” of some values (e.g., results of the Bechdel-Wallace test), we still ask you to test whether or not the proportion of episodes that pass the test is equal to 0.4. This may seem counter-intuitive, but you can think of it as follows:\n",
        "\n",
        "- The episode test results are treated as realisations from an unknown probability distribution $f$ (here, Bernoulli(p)).\n",
        "- Although the episodes are released, we are interested in the underlying process that generates these values. This includes not-yet-released episodes or hypothetical similar episodes. Simply examining the “complete” population of Bechdel test results is not sufficient; instead, we rely on a statistical model to quantify uncertainty."
      ],
      "metadata": {
        "id": "QnaGxyE1kJmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H0: p = 0.4\n",
        "#H1: p != 0.4\n",
        "\n",
        "episodes %>%\n",
        "  filter(Series == \"TNG\") %>%\n",
        "  summarise(p_hat = mean(Bechdel.Wallace.Test),\n",
        "            n = n(),\n",
        "            p_H0 = 0.4,\n",
        "            z_975 = qnorm(0.975)) %>%\n",
        "  mutate(z_obs = abs((p_hat-p_H0)/sqrt(p_H0*(1-p_H0)/n)))\n",
        "\n",
        "\n",
        "#As abc(z_obs) < z_975, at 95% significance level, there is no evidence against the null hypothesis that\n",
        "#the proportion of TNG episodes that pass the Bechdel-Wallace Test is 0.4. Do not reject the null hypothesis."
      ],
      "metadata": {
        "id": "q1nduV1CjAUo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "98bcc54b-2385-4dd6-bb1f-c0054967889c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 1 × 5</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>p_hat</th><th scope=col>n</th><th scope=col>p_H0</th><th scope=col>z_975</th><th scope=col>z_obs</th></tr>\n",
              "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td>0.4382022</td><td>178</td><td>0.4</td><td>1.959964</td><td>1.040383</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 1 × 5\n\n| p_hat &lt;dbl&gt; | n &lt;int&gt; | p_H0 &lt;dbl&gt; | z_975 &lt;dbl&gt; | z_obs &lt;dbl&gt; |\n|---|---|---|---|---|\n| 0.4382022 | 178 | 0.4 | 1.959964 | 1.040383 |\n\n",
            "text/latex": "A data.frame: 1 × 5\n\\begin{tabular}{lllll}\n p\\_hat & n & p\\_H0 & z\\_975 & z\\_obs\\\\\n <dbl> & <int> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t 0.4382022 & 178 & 0.4 & 1.959964 & 1.040383\\\\\n\\end{tabular}\n",
            "text/plain": [
              "  p_hat     n   p_H0 z_975    z_obs   \n",
              "1 0.4382022 178 0.4  1.959964 1.040383"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 1.4**\n",
        "\n",
        "\n",
        "Is there evidence that the proportion of episodes that pass the Bechdel-Wallace Test differs between Star Trek: The Next Generation and Star Trek: Voyager? Interpret the results for a non-statistician stakeholder."
      ],
      "metadata": {
        "id": "FoGVfo_hjArg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H0: p_TOS - p_TNG = 0\n",
        "#H1: p_TOS - p_TNG != 0\n",
        "\n",
        "episodes %>%\n",
        "  filter(Series %in%  c(\"TOS\", \"TNG\")) %>%\n",
        "  group_by(Series) %>%\n",
        "  summarise(p_hat = mean(Bechdel.Wallace.Test),\n",
        "            n = n()) -> summaryStats\n",
        "\n",
        "TNG_stats = summaryStats %>% filter(Series == \"TNG\")\n",
        "TOS_stats = summaryStats %>% filter(Series == \"TOS\")\n",
        "\n",
        "z_obs = (TOS_stats$p_hat - TNG_stats$p_hat)/sqrt(TOS_stats$p_hat*(1-TOS_stats$p_hat)/TOS_stats$n + TNG_stats$p_hat*(1-TNG_stats$p_hat)/TNG_stats$n)\n",
        "z_975 = qnorm(0.975)\n",
        "\n",
        "abs(z_obs) > z_975\n",
        "\n",
        "#As abc(z_obs) > z_975, at 95% significance level, there is evidence against the null hypothesis that\n",
        "#the proportion of episodes passing the Bechdel test of TOS is equal to that of TNG. Reject the null hypothesis in favour of the\n",
        "#alternative one that these two proportions are different."
      ],
      "metadata": {
        "id": "8azh6dZHjeDc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8050743b-db76-4895-83eb-ed611e836b91"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "TRUE"
            ],
            "text/markdown": "TRUE",
            "text/latex": "TRUE",
            "text/plain": [
              "[1] TRUE"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 2**\n",
        "\n",
        "The following questions are based on the `epa_data` dataset. While you are expected to use R to compute the answers, the underlying concepts are identical to those in pen-and-paper hypothesis testing calculations.\n"
      ],
      "metadata": {
        "id": "iABy0Dkejia4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epa_data = read.csv(\"./datasets/epa_data.csv\")\n",
        "epa_data %>% str()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQgqS1IXjzMp",
        "outputId": "95dfe214-db3f-4387-81c8-87a44c0a3d14"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'data.frame':\t13569 obs. of  9 variables:\n",
            " $ city : int  16 15 16 19 19 19 19 19 19 19 ...\n",
            " $ hwy  : int  24 22 22 27 29 24 26 27 29 24 ...\n",
            " $ cyl  : int  8 8 8 4 4 4 4 4 4 4 ...\n",
            " $ disp : num  5 5 5 2 2 2.4 2.4 2 2 2.4 ...\n",
            " $ drive: chr  \"Rear-Wheel Drive\" \"Rear-Wheel Drive\" \"Rear-Wheel Drive\" \"Rear-Wheel Drive\" ...\n",
            " $ make : chr  \"Jaguar\" \"Jaguar\" \"Jaguar\" \"Pontiac\" ...\n",
            " $ model: chr  \"XK\" \"XK\" \"XK Convertible\" \"Solstice\" ...\n",
            " $ trans: chr  \"Automatic\" \"Automatic\" \"Automatic\" \"Automatic\" ...\n",
            " $ year : int  2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 2.1**\n",
        "\n",
        "Is there evidence of a difference in the average city mileage between cars manufactured in 2015 and 2020?  Interpret the results for a non-statistician stakeholder."
      ],
      "metadata": {
        "id": "zisuB0lUj3yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H0: mu_2020 - mu_2015 = 0\n",
        "#H1: mu_2020 - mu_2015 != 0\n",
        "\n",
        "epa_data %>%\n",
        "  filter(year %in%  c(2015, 2020)) %>%\n",
        "  group_by(year) %>%\n",
        "  summarise(avgMileage = mean(city),\n",
        "            varMileage = var(city),\n",
        "            n = n()) -> summaryStats\n",
        "\n",
        "epa_2015 = summaryStats %>% filter(year == 2015)\n",
        "epa_2020 = summaryStats %>% filter(year == 2020)\n",
        "\n",
        "z_obs = (epa_2020$avgMileage - epa_2015$avgMileage)/sqrt(epa_2020$varMileage/epa_2020$n + epa_2015$varMileage/epa_2015$n)\n",
        "z_975 = qnorm(0.975)\n",
        "\n",
        "abs(z_obs) > z_975\n",
        "\n",
        "#As abc(z_obs) > z_975, at 95% significance level, there is evidence against the null hypothesis that\n",
        "#there is no difference between the average city milages between cars manufactured of TOS in 2015 and 2020. Reject the null hypothesis."
      ],
      "metadata": {
        "id": "F9gnd1ydlHxo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1b1b9df3-0731-4d61-b75b-fff728825f0d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "TRUE"
            ],
            "text/markdown": "TRUE",
            "text/latex": "TRUE",
            "text/plain": [
              "[1] TRUE"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "Solution will be released at the end of the week!\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "9XFHEzV3lJSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 2.2**\n",
        "\n",
        "Is there evidence that the proportion of cars produced with manual transmissions in 2010 is greater than 0.5? Interpret the results for a non-statistician stakeholder."
      ],
      "metadata": {
        "id": "9e6ODNN6lKpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H0: p = 0.5\n",
        "#H1: p > 0.5\n",
        "\n",
        "\n",
        "epa_data %>%\n",
        "  filter(year == 2010) %>%\n",
        "  summarise(p_hat = mean(ifelse(trans == \"Manual\", 1, 0)),\n",
        "            n = n(),\n",
        "            p_H0 = 0.5,\n",
        "            z_95 = qnorm(0.95)) %>%\n",
        "  mutate(z_obs = (p_hat-p_H0)/sqrt(p_H0*(1-p_H0)/n))\n",
        "\n",
        "#As z_obs < z_95, at 95% significance level, there is no evidence against the null hypothesis that\n",
        "#the proportion of cars produced with manual transmissions in 2010 is 0.5. Do not reject the null hypothesis."
      ],
      "metadata": {
        "id": "BqQFbnEjlXoh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "1b3f0632-5075-4755-8212-d448eaeb586c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 1 × 5</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>p_hat</th><th scope=col>n</th><th scope=col>p_H0</th><th scope=col>z_95</th><th scope=col>z_obs</th></tr>\n",
              "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td>0.2434626</td><td>1109</td><td>0.5</td><td>1.644854</td><td>-17.08624</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 1 × 5\n\n| p_hat &lt;dbl&gt; | n &lt;int&gt; | p_H0 &lt;dbl&gt; | z_95 &lt;dbl&gt; | z_obs &lt;dbl&gt; |\n|---|---|---|---|---|\n| 0.2434626 | 1109 | 0.5 | 1.644854 | -17.08624 |\n\n",
            "text/latex": "A data.frame: 1 × 5\n\\begin{tabular}{lllll}\n p\\_hat & n & p\\_H0 & z\\_95 & z\\_obs\\\\\n <dbl> & <int> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t 0.2434626 & 1109 & 0.5 & 1.644854 & -17.08624\\\\\n\\end{tabular}\n",
            "text/plain": [
              "  p_hat     n    p_H0 z_95     z_obs    \n",
              "1 0.2434626 1109 0.5  1.644854 -17.08624"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "Solution will be released at the end of the week!\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "my6tFYsOlkat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 2.3**\n",
        "\n",
        "Is there evidence that the proportion of cars produced with manual transmissions for the years 2011 and 2012 has decreased? Interpret the results for a non-statistician stakeholder."
      ],
      "metadata": {
        "id": "fNu9mI0rlk0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H0: mu_2012 - mu_2011 = 0\n",
        "#H1: mu_2012 - mu_2011 < 0\n",
        "\n",
        "epa_data %>%\n",
        "  filter(year %in%  c(2012, 2011)) %>%\n",
        "  group_by(year) %>%\n",
        "  summarise(p_hat = mean(ifelse(trans == \"Manual\", 1, 0)),\n",
        "            n = n()) -> summaryStats\n",
        "\n",
        "epa_2012 = summaryStats %>% filter(year == 2012)\n",
        "epa_2011 = summaryStats %>% filter(year == 2011)\n",
        "\n",
        "z_obs = (epa_2012$p_hat - epa_2011$p_hat)/sqrt(epa_2012$p_hat*(1-epa_2012$p_hat)/epa_2012$n + epa_2011$p_hat*(1-epa_2011$p_hat)/epa_2011$n)\n",
        "z_05 = qnorm(0.05)\n",
        "\n",
        "z_obs < z_05\n",
        "\n",
        "#As z_obs > z_05, at 95% significance level, there is no evidence against the null hypothesis that the proportion of cars\n",
        "# produced with manual transmissions for the years 2011 and 2012 has remained the same. Do not reject the null hypothesis.\n"
      ],
      "metadata": {
        "id": "8txY_R41lsJP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "988eb75a-a553-4c09-8bbb-a36110c278c4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "FALSE"
            ],
            "text/markdown": "FALSE",
            "text/latex": "FALSE",
            "text/plain": [
              "[1] FALSE"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "Solution will be released at the end of the week!\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "G2Sk-1I2ls-H"
      }
    }
  ]
}