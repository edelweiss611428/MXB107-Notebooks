{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Week 11: Simple Linear Regression**\n",
        "\n",
        "```\n",
        ".------------------------------------.\n",
        "|   __  ____  ______  _  ___ _____   |\n",
        "|  |  \\/  \\ \\/ / __ )/ |/ _ \\___  |  |\n",
        "|  | |\\/| |\\  /|  _ \\| | | | | / /   |\n",
        "|  | |  | |/  \\| |_) | | |_| |/ /    |\n",
        "|  |_|  |_/_/\\_\\____/|_|\\___//_/     |\n",
        "'------------------------------------'\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "ZtpJJ8BSHOaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this workshop, we will explore how to perform linear regression in R through practical exercises."
      ],
      "metadata": {
        "id": "nJN_EjsDJ5My"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-Configurating the Notebook**"
      ],
      "metadata": {
        "id": "qJMHamzBgYY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Switching to the R Kernel on Colab**\n",
        "\n",
        "By default, Google Colab uses Python as its programming language. To use R instead, you’ll need to manually switch the kernel by going to **Runtime > Change runtime type**, and selecting R as the kernel. This allows you to run R code in the Colab environment.\n",
        "\n",
        "However, our notebook is already configured to use R by default. Unless something goes wrong, you shouldn’t need to manually change runtime type."
      ],
      "metadata": {
        "id": "RYNbU2mDgjJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing Required Packages**\n",
        "**Run the following lines of code**:"
      ],
      "metadata": {
        "id": "Sh6s1ChmglUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do not modify\n",
        "\n",
        "setwd(\"/content\")\n",
        "\n",
        "# Remove `MXB107-Notebooks` if exists,\n",
        "if (dir.exists(\"MXB107-Notebooks\")) {\n",
        "  system(\"rm -rf MXB107-Notebooks\")\n",
        "}\n",
        "\n",
        "# Fork the repository\n",
        "system(\"git clone https://github.com/edelweiss611428/MXB107-Notebooks.git\")\n",
        "\n",
        "# Change working directory to \"MXB107-Notebooks\"\n",
        "setwd(\"MXB107-Notebooks\")\n",
        "\n",
        "#\n",
        "invisible(source(\"R/preConfigurated.R\"))"
      ],
      "metadata": {
        "id": "hXjYwRwxgmqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do not modify the following**"
      ],
      "metadata": {
        "id": "0xU1keysgotb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if (!require(\"testthat\")) install.packages(\"testthat\"); library(\"testthat\")\n",
        "\n",
        "test_that(\"Test if all packages have been loaded\", {\n",
        "\n",
        "  expect_true(all(c(\"ggplot2\", \"tidyr\", \"dplyr\", \"stringr\", \"magrittr\", \"knitr\") %in% loadedNamespaces()))\n",
        "\n",
        "})"
      ],
      "metadata": {
        "id": "VkOd7qZXgqr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simple Linear Regression Model**\n",
        "\n",
        "We have already introduced simple linear regression in the bivariate data summary workshop/lecture. This workshop content will go a bit deeper and focus on:\n",
        "\n",
        "- Performing hypothesis tests for regression parameters\n",
        "- Interpreting key quantities from `lm()` model outputs\n",
        "- Computing confidence intervals and prediction intervals\n",
        "- Interpreting diagnostic plots to assess model fit\n"
      ],
      "metadata": {
        "id": "_EVVevuQgTtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fitting Simple Linear Regression Models in R**\n",
        "\n",
        "R provides the `lm()` function for fitting linear regression models. It has a formula interface, similar to `aov()`, `t.test()`, and other modeling functions in R. Linear models are very flexible and can be used for simple regression, multiple regression, and even ANOVA (since ANOVA is a special case of a linear model).\n",
        "\n"
      ],
      "metadata": {
        "id": "0RHRPsiKmVJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Usage:**\n",
        "\n",
        "```r\n",
        "lm(formula,\n",
        "   data = NULL,\n",
        "   subset = NULL,\n",
        "   weights = NULL,\n",
        "   na.action = na.omit,\n",
        "   ...)\n",
        "```"
      ],
      "metadata": {
        "id": "s5q_aJeJn6ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Arguments:**\n",
        "\n",
        "- `formula`: a model formula of the form `response ~ predictors` (e.g., `y ~ x` for simple linear regression.)  \n",
        "- `data`: a data frame containing the variables in the model  \n",
        "- `subset`: an optional vector specifying a subset of observations to be used  \n",
        "- `weights`: an optional vector of weights for weighted regression  \n",
        "- `na.action`: a function that indicates what should happen when the data contain `NA`s (default is `na.omit`)  \n",
        "- `...`: additional arguments passed to lower-level modeling functions  "
      ],
      "metadata": {
        "id": "8sFv6_F-n7-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **\"Reading\" `lm` Objects**\n",
        "\n",
        "The output of the `lm()` function is an object of class `lm`. By default, printing this object displays only basic information, such as the call and the estimated coefficients.\n",
        "\n",
        "Consider the `cars` example in `Mock-PST2.docx`. We are interested in studying the relationship between the speed of cars (in mph) and the distance required to stop (in feet). To explore this relationship, we may fit a linear model using `lm()`."
      ],
      "metadata": {
        "id": "6PhcKMinoB4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = lm(dist~speed, cars)\n",
        "class(model1)\n",
        "model1"
      ],
      "metadata": {
        "id": "RKCmrwrxoaIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like objects returned by `t.test()` and `aov()`, an `lm` object can be summarised using `summary()` to obtain additional statistics, such as standard errors, t-values, and p-values."
      ],
      "metadata": {
        "id": "8GGXULuupvr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model1)"
      ],
      "metadata": {
        "id": "cyNDH3V0p-Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise**\n",
        "\n",
        "From the summary output of the `lm` object, answer the following questions:\n",
        "\n",
        "- How much variability in stopping distance is explained by the linear model?  \n",
        "- Interpret the *estimated* coefficient for `speed`. Is there any evidence that the linear relationship between speed and `dist` is statistically significant?\n"
      ],
      "metadata": {
        "id": "khtDSaG8qt_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "The amount of variability in stopping distance explained by the linear model is R squared, 65.11%.\n",
        "\n",
        "The estimated coefficient for `speed` is 3.9324, implying that an additional mph in speed requires 3.9324 more feet to stop. This coefficient is statistically significant, as the p-value from the t-test is extremely small and smaller than any conventional threshold (e.g., 0.05).\n",
        "\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "OUZW9CFCrpHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Also, note that the F-test from the ANOVA of the model shows the same result. Is this always true?  \n",
        "\n",
        "- **Yes**, for simple linear regression models.  \n",
        "- **No**, for multiple linear regression models (out of the scope of this unit).  \n",
        "\n",
        "In a regression model, the F-test is used to test whether all regression coefficients (excluding the intercept) are simultaneously equal to zero. For simple linear regression, since there is only one predictor, the F-test and the t-test for that coefficient are provably mathematically equivalent. One may obtain the F-test output using the `anova()` function."
      ],
      "metadata": {
        "id": "m1Vwv1amsr7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anova(model1)"
      ],
      "metadata": {
        "id": "xFQ-t7-46Hdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or equivalently,"
      ],
      "metadata": {
        "id": "_4kOzMHYO44p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aov(dist~speed, cars) %>% summary()"
      ],
      "metadata": {
        "id": "UqpRQ8KGO6Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHVYAqIpdaCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Various useful statistics can be extracted from `lm` objects or summarised `lm` objects."
      ],
      "metadata": {
        "id": "pfmUIyKtdpqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lm object\n",
        "model1$coefficients       # coefficients\n",
        "#model1$fitted.values      # predicted values\n",
        "#model1$residuals          # residuals\n",
        "#model1$df.residual        # residual df\n",
        "#model1$rank               # rank of model\n",
        "\n",
        "# summary object\n",
        "s1 = summary(model1)\n",
        "cat(\"--Coefs--\\n\")\n",
        "s1$coefficients           # estimates + SE + t + p\n",
        "cat(\"--sigma--\\n\")\n",
        "s1$sigma                  # residual standard error\n",
        "cat(\"--Rsq--\\n\")\n",
        "s1$r.squared              # R-squared\n",
        "cat(\"--ADjRsq--\\n\")\n",
        "s1$adj.r.squared          # adjusted R-squared\n",
        "cat(\"--Fstats--\\n\")\n",
        "s1$fstatistic             # F-statistic"
      ],
      "metadata": {
        "id": "nRSuEDzAd3fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interval Estimation**\n",
        "\n",
        "In linear regression, we often want to quantify the uncertainty in our predictions or in the estimated coefficients. Here, we will explore several common ones.\n"
      ],
      "metadata": {
        "id": "KL3Sp_qht1dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Confidence Intervals for Regression Coefficients**  \n",
        "This confidence interval reflects the uncertainty in estimating the relationship between the predictor and the response.  In R, you can compute confidence intervals for model coefficients using the `confint()` function:"
      ],
      "metadata": {
        "id": "D2VoBJS96b41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  confint(model1, level = 0.95)"
      ],
      "metadata": {
        "id": "Df2S3fTf6zXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Confidence Intervals for Mean Response**  \n",
        "\n",
        "This confidence interval reflects the uncertainty in estimating the **expected response** conditioned on a regressor value. This expectation is a **FIXED** value.\n",
        "\n",
        "In R, you can compute confidence intervals for the mean response using the `predict()` function with `interval = \"confidence\"`."
      ],
      "metadata": {
        "id": "3STJy87y66v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model1, newdata = data.frame(speed = 20), interval = \"confidence\",  level = 0.95)"
      ],
      "metadata": {
        "id": "u6bOwz5b7WzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, if `newdata` is not supplied, the `predict()` function uses the predictor values from the original dataset (the ordering is preserved).\n"
      ],
      "metadata": {
        "id": "Blk_2gzs7bx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1$fitted.values %>% head(10) %>% kable()\n",
        "predict(model1, interval = \"confidence\") %>% head(10) %>% kable()"
      ],
      "metadata": {
        "id": "xB5apCN17XoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember `geom_smooth`? In `geom_smooth()`, the shaded area is by default a confidence interval for the mean response."
      ],
      "metadata": {
        "id": "C5iYa6za8F5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cars %>%\n",
        "  ggplot(aes(x = speed, y = dist)) +\n",
        "  geom_point()+\n",
        "  geom_smooth(method = \"lm\")"
      ],
      "metadata": {
        "id": "8kEM7ZZr8PAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Predictive Intervals for *Future* Observations**\n",
        "\n",
        "This is an entirely different concept from confidence intervals. Previously, when we talked about confidence intervals, we were estimating intervals for **fixed quantities** (e.g., the mean response).  \n",
        "\n",
        "Predictive intervals, on the other hand, are for **random quantities** — they give a range where a future observation is likely to fall, with a specified coverage probability **before seeing the data**.  \n",
        "\n",
        "For example, we may want to predict the value of `dist` for a new car with `speed = 20`. Here, we are not interested in the **average** `dist` but rather the value of `dist` for this specific case, which is a **RANDOM** quantity. That is why we are **predicting**, not estimating.\n"
      ],
      "metadata": {
        "id": "YWiZcr4K8cIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In R, you can compute confidence intervals for the mean response using the `predict()` function with `interval = \"prediction\"`."
      ],
      "metadata": {
        "id": "RxcqHoDV9cNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model1, newdata = data.frame(speed = 20), interval = \"confidence\",  level = 0.95)\n",
        "predict(model1, newdata = data.frame(speed = 20), interval = \"prediction\",  level = 0.95)"
      ],
      "metadata": {
        "id": "DF055ERw9RpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the prediction intervals are wider; this is to account for additional uncertainty due to the randomness of the response variable."
      ],
      "metadata": {
        "id": "jwKI-fGZ9fJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regression Diagnostics**\n",
        "\n",
        "> Anscombe's quartet comprises four datasets that have nearly identical simple  descriptive statistics, yet have very different distributions and appear very different when graphed.\n",
        "\n",
        "From [Anscombe's quartet - Wikipedia](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)\n",
        "\n"
      ],
      "metadata": {
        "id": "lkyG_EkBqHom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "par(mfrow = c(2,2))\n",
        "\n",
        "plot(anscombe$x1,anscombe$y1)\n",
        "plot(anscombe$x2,anscombe$y2)\n",
        "plot(anscombe$x3,anscombe$y3)\n",
        "plot(anscombe$x4,anscombe$y4)\n",
        "\n",
        "par(mfrow = c(1,1))"
      ],
      "metadata": {
        "id": "rmVLZCctt6Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm(y1~x1, anscombe) %>% summary()\n",
        "cat(\"-------------------------------------------- \\n\")\n",
        "lm(y2~x2, anscombe) %>% summary()\n",
        "cat(\"-------------------------------------------- \\n\")\n",
        "lm(y3~x3, anscombe) %>% summary()\n",
        "cat(\"-------------------------------------------- \\n\")\n",
        "lm(y4~x4, anscombe) %>% summary()"
      ],
      "metadata": {
        "id": "GxBEs7Tpuhyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These datasets have nearly identical simple descriptive statistics, yet, only the first dataset `(x1, y1)` resembles a linear relationship.\n",
        "\n",
        "**However, many of the tools we have learnt in this class, such as sampling distributions of regression coefficients, hypothesis testing, and interval estimation are only reliable if regression assumptions are adequate (i.e., no substantial deviation from linear regression assumptions)**.\n",
        "\n",
        "This highlights the importance of checking assumptions through regression diagnostics. In this unit, we mostly focus on the following assumptions: linearity, homoskedasticity, and normality."
      ],
      "metadata": {
        "id": "434VZ2RVwXWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Linearity**\n",
        "\n",
        "The simplest way to check the linearity assumption is through a scatter plot. One can plot the relationship between the variables and overlay the fitted regression line. In simple regression, non-linearity is usually easy to detect visually. For multiple regression, it becomes more difficult to assess linearity directly.\n"
      ],
      "metadata": {
        "id": "dngf-oqryDmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, in the `cars` dataset, it is clear that as `speed` increases, `dist` tends to increase approximately linearly. The fitted regression line captures this overall relationship, and the data points are scattered around the line, indicating a reasonable fit.\n"
      ],
      "metadata": {
        "id": "XJVoVtRky2_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cars %>%\n",
        "  ggplot(aes(x = speed, y = dist)) +\n",
        "  geom_point() +\n",
        "  geom_smooth(method = \"lm\")"
      ],
      "metadata": {
        "id": "gDSyRaTsycqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike the `cars` example, in `(x2,y2)` dataset in the `anscombe` example, the scatter plot resembles a quadratic relationship rather than a linear one. The fitted line does not capture the relationship between `x2` and `y2`."
      ],
      "metadata": {
        "id": "oVXeQx_XzSmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anscombe %>%\n",
        "  ggplot(aes(x = x2, y = y2)) +\n",
        "  geom_point() +\n",
        "  geom_smooth(method = \"lm\")"
      ],
      "metadata": {
        "id": "3wq3VVGzytEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the assumptions of linear regression hold, we expect the data points to scatter around the fitted line. Another way to check for linearity is by plotting the residuals against the fitted values. If the linearity assumption is reasonable, the residuals should scatter randomly around the horizontal line at 0, regardless of the fitted values.\n",
        "\n",
        "We can extract residuals & fitted values from an `lm` object and manually create a residuals vs. fitted values plot.\n"
      ],
      "metadata": {
        "id": "fmwfZt8czhuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = lm(dist~speed, cars)\n",
        "plot(model1$fitted.values, model1$residuals)\n",
        "abline(h = 0, col = \"red\")"
      ],
      "metadata": {
        "id": "84H9yTRyz6Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residuals seem to scatter around 0 here."
      ],
      "metadata": {
        "id": "a8U0L_PM0XDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another option (more recommended in practice) is to use built-in methods in R for regression diagnostics. We can simply run `plot(model1, which = 1)`. Here, `which = 1` tells `R` that you want a residuals vs fitted values plot.\n",
        "\n",
        "This plot also includes a smooth curve (a moving average line) that shows the local average of the residuals, helping to detect any systematic deviations from linearity."
      ],
      "metadata": {
        "id": "k78YsO5N0qXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot(model1, which = 1)"
      ],
      "metadata": {
        "id": "5Cntbioa0xHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that there is a minor deviation from the linearity assumption. Such deviations can be difficult to detect if you only consider a manually created residuals vs. fitted plot or a scatter plot with the fitted line.\n",
        "\n",
        "Overall, the linearity assumption seems reasonable here. (Note that in practice, there is generally no \"true\" linear relationship; rather, we aim for a relationship that can be well approximated by a linear model.)\n"
      ],
      "metadata": {
        "id": "ZYMm8fnm1NEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Homoskedasticity**\n",
        "\n",
        "Another important assumption of the classical linear regression model is homoskedasticity. It states that the variance of the errors (residuals) is constant across all levels of the predictor(s). A simple way to check this is to examine the residuals vs. fitted values plot or even the original scatter plot. If you notice that the spread of the residuals changes systematically with the fitted values, this indicates a potential violation of the homoskedasticity assumption.\n",
        "\n",
        "Back to the `cars` example, here, it seems that spread of the residuals slightly increases as the fitted value increases. This suggests heteroskedasticity.\n",
        "\n",
        "In regression literature, there is a dedicated diagnostic plot for checking homoskedasticity called the **scale-location plot**. You can generate it using `plot(model1, which = 3)`. The theory behind this is out of the scope of this unit. If homoskedasticity and linearity hold, the moving average line should be roughly horizontal. Here, it is not, suggesting heteroskedasticity.\n"
      ],
      "metadata": {
        "id": "A1zbSPpi2BRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot(model1, which = 3)"
      ],
      "metadata": {
        "id": "NWlpZBaf3MSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Normality**"
      ],
      "metadata": {
        "id": "i5B_805F3x5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The residuals should be roughly Gaussian if regression assumptions hold. As a results, we may use `qqplot` to check for normality."
      ],
      "metadata": {
        "id": "-ccJ1SdO4Gwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qqnorm(model1$residuals)\n",
        "qqline(model1$residuals)"
      ],
      "metadata": {
        "id": "BZMkosrD4SaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the Q-Q plot, most points lie roughly along the diagonal line, although a few extreme values deviate from it. This indicates a slight departure from normality."
      ],
      "metadata": {
        "id": "m9FsKWC84cka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use `plot(model1, which = 2)` to generate a similar plot; however, here, residuals are standardised - we won't go into much detail how these have been standardised as this is not the one we learn in this unit. An advantage of this is that it let us convert the scale of the residuals to be that of the standard Gaussian, which makes it easier to interpret (especially when there are outliers)."
      ],
      "metadata": {
        "id": "493XsXCh4dae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot(model1, which = 2)"
      ],
      "metadata": {
        "id": "ZcUvVJA34pJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Workshop Questions**\n",
        "\n"
      ],
      "metadata": {
        "id": "tn0yEx9h_t26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 1: Simpson's Paradox**\n",
        "\n",
        "\n",
        "> Simpson's paradox is a phenomenon in probability and statistics in which a trend appears in several groups of data but disappears or reverses when the groups are combined. This result is often encountered in social-science and medical-science statistics, and is particularly problematic when frequency data are unduly given causal interpretations.\n",
        "\n",
        "From [Simpson's Paradox - Wikipedia](https://en.wikipedia.org/wiki/Simpson%27s_paradox)\n",
        "\n",
        "\n",
        "The following synthetic dataset `typo` (inspired by [Point estimates, Simpson’s paradox, and nonergodicity in biological sciences - Science Direct](https://www.sciencedirect.com/science/article/pii/S0149763421000713?ref=cra_js_challenge&fr=RR-1)) contains information about word per minute (`wpm`) and the number of typos (`typo`) of five typers.\n",
        "\n"
      ],
      "metadata": {
        "id": "CbnQ2iIfAW8j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HfKX0-t_BKhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "typo = read.csv(\"./datasets/typo.csv\")\n",
        "typo %>% str()"
      ],
      "metadata": {
        "id": "twxpzYINBG6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 1.1**\n",
        "\n",
        "We are interested in the linear relationship `typo ~ wpm`. Fit a linear model using the `lm()` function and summarise the result. Interpret the regression coefficient (and the corresponding hypothesis testing output). Does it make any sense?"
      ],
      "metadata": {
        "id": "I0Mk-TTcBcs6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6pceJlqtB15O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "```r\n",
        "lm(typo~wpm, typo) -> model3\n",
        "summary(model3)\n",
        "```\n",
        "\n",
        "The p-value associated to `wpm` is extremely small, suggesting a significant linear relationship between `typo` and `wpm`. The estimated coefficient is roughly -0.097, implying that for an additional `wpm`, a typer typically reduce the number of errors by roughly 0.097, which does not make any sense.\n",
        "\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "eCXjTfzwCZoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 1.2**\n",
        "\n",
        "Create a scatter plot of `typo` vs. `wpm` with fitted `lm` lines, segmented (coloured) by `typer`. What do you observe?"
      ],
      "metadata": {
        "id": "75jzZ-5NDOTD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7b9Ohr1oDkDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "```r\n",
        "typo %>%\n",
        "  ggplot(aes(x = wpm, y = typo, colour = typer)) +\n",
        "  geom_point() +\n",
        "  geom_smooth(method = \"lm\")\n",
        "\n",
        "```\n",
        "\n",
        "If we do not take into account `typer`, it appears that as `wpm` increases, `typo` decreases, which does not make sense. However, within each group (`typer`), the direction of the linear relationship reverses - a classic example of Simpson's paradox.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "BllQKiYJDljT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 1.3**\n",
        "\n",
        "One may be interested in running a **post-hoc ANCOVA** to check if there are significant differences in `typo` across `typer`, **after controlling for `wpm`**.  Perform ANCOVA in R and interpret the summarised results.\n",
        "\n",
        "**Hint**: You can use the `aov()` function (similar to ANOVA) on the model `typo ~ wpm + typer` (without interaction) then summarise the output."
      ],
      "metadata": {
        "id": "L4AHY9ZdEznC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nf0_8XQyFqQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "```r\n",
        "\n",
        "model4 = aov(typo ~ wpm + typer, data = typo)\n",
        "summary(model4)\n",
        "```\n",
        "\n",
        "The extremely small p-value associated with typer indicates that there are significant differences in the number of `typo` across different `typer` after controlling for `wpm`.\n",
        "\n",
        "Similarly, the very small p-value for `wpm` suggests a significant linear relationship between `wpm` and `typo`, even after adjusting for `typer`.\n",
        "\n",
        "Of course, sometimes after controlling for one variable, you may find no remaining linear relationship in another predictor - even if, when fitted alone, it appeared to have a strong association. This highlights the importance of considering multiple predictors together. You will see more about multiple linear regression in future units.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "WcvolCGMF_-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 1.4**\n",
        "\n",
        "ANCOVA is a special case of the (multiple) linear regression model, which means the theory behind diagnostic plots such as the residuals vs. fitted values plot and the Q–Q plot for Gaussianity still holds.\n",
        "(You do not need to know the technical details of these plots to answer this question.)\n",
        "\n",
        "Comment on the residuals vs. fitted plot before including `typer`(simple regression) and after including `typer` (ANCOVA)."
      ],
      "metadata": {
        "id": "XhsC5w-aMMlx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFD3IjVGNBzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "```r\n",
        "lm(typo~wpm, typo) -> model3\n",
        "plot(model3, which = 1)\n",
        "model4 = aov(typo ~ wpm + typer, data = typo)\n",
        "plot(model4, which = 1)\n",
        "```\n",
        "\n",
        "Including `typer` removes the unexplained non-linear pattern in the residuals vs. fitted plot due to difference in `typo` across groups.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "De8gIomXNC2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 2**\n",
        "\n",
        "The dataset `sapphire` features Young’s modulus (`Young`) measured at various temperatures (`Temp`) for sapphire rods.\n",
        "\n"
      ],
      "metadata": {
        "id": "8QhnZcr7G7F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sapphire = read.csv(\"./datasets/sapphire.csv\")\n",
        "sapphire %>% str()"
      ],
      "metadata": {
        "id": "vpwTapwZI5A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 2.1**\n",
        "\n",
        "Complete the following tasks:\n",
        "\n",
        "- Fit a linear relationship `Young~Temp`. Is the linear relationship statistically significant?\n",
        "- How much variablility in `Young` is explained by the linear relationship with `Temp`?\n",
        "- Compute 99% (symmetric) confidence intervals for the parameter estimates."
      ],
      "metadata": {
        "id": "k-6cgeT8J28k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JnvvwOjjJNJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "```r\n",
        "lm(Young~Temp, sapphire) -> model5\n",
        "summary(model5)\n",
        "```\n",
        "\n",
        "The p-value associated to `Temp` is extremely small, suggesting a significant linear relationship between `Young` and `Temp`.\n",
        "\n",
        "R-squared = 98.22%, implying 98.22% of variability in `Young` is explained by the linear relationship with `Temp`\n",
        "\n",
        "```r\n",
        "#Confidence intervals for model parameters\n",
        "confint(model5, level = 0.99)\n",
        "```\n",
        "\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "AHfCnoU1I5aS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 2.2**"
      ],
      "metadata": {
        "id": "nEHQLUKIJ_vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform regression diagnostics using techniques presented in this workshop."
      ],
      "metadata": {
        "id": "CChu4P8lJ93Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLrYU3y7KSe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "```r\n",
        "lm(Young~Temp, sapphire) -> model5\n",
        "\n",
        "sapphire %>%\n",
        "  ggplot(aes(x = Temp, y = Young)) +\n",
        "  geom_point()+\n",
        "  geom_smooth(method = \"lm\")\n",
        "\n",
        "plot(model5, which = 1)\n",
        "plot(model5, which = 2)\n",
        "```\n",
        "\n",
        "As the residuals are scattered around 0, and the scatter plot between `Young` and `Temp` seems to resemble a linear relationship, the linearity assumption appears to be adequate.\n",
        "\n",
        "From the previous two plots, there seems to be no clear violation of the homoskedasticity assumption. However, the data appear to be bi-level rather than “normally” scattered around the regression line, suggesting non-normality.\n",
        "\n",
        "Indeed, from the Q–Q plot, we can observe a clear violation of the Gaussian assumption, as most points do not lie on the theoretical quantile line.\n",
        "\n",
        "Overall, the linear regression model can reasonably be applied here, but caution is advised due to the extreme departure from normality. There might also be unexplained factors due to the bi-level pattern.\n",
        "\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "CpNsJhzhKeyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 3**\n",
        "\n",
        "Sometimes the linear relationship does not appear on the original (linear) scale of data but only after applying a suitable non-linear transformation.  \n",
        "\n",
        "\n",
        "A classic example is **Newton’s law of cooling**, which states:\n",
        "$$\n",
        "T(t) = T_{\\text{env}} + (T_0 - T_{\\text{env}}) e^{-kt},\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- $T(t)$ is the temperature of the object at time $t$,  \n",
        "- $T_{\\text{env}}$ is the ambient temperature,  \n",
        "- $T_0$ is the initial temperature,  \n",
        "- $k$ is a positive constant.  \n",
        "\n",
        "This formula is **not linear** in its raw form. However, with a simple transformation:\n",
        "\n",
        "$$\n",
        "\\ln \\big(T(t) - T_{\\text{env}}\\big) = \\ln(T_0 - T_{\\text{env}}) - kt,\n",
        "$$\n",
        "\n",
        "the relationship between $\\ln(T(t) - T_{\\text{env}})$ and $t$ becomes **linear**.  \n"
      ],
      "metadata": {
        "id": "3C179kKoO1-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 3.1**\n",
        "\n",
        "Given the following dataset `cooling`, which stores:\n",
        "\n",
        "- `time`: the measurement times,  \n",
        "- `temperature`: the observed temperatures (with possible measurement error),  \n",
        "- parameters such as the ambient temperature `Tenv` and initial temperature `T0`,  \n",
        "\n",
        "could you use a **linear regression model** to estimate the decay constant $k$ in Newton’s law of cooling?\n"
      ],
      "metadata": {
        "id": "Dsyze1bPbrmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cooling = read.csv(\"./datasets/cooling.csv\")\n",
        "cooling %>% str()\n",
        "Tenv = 20   # ambient temperature (°C)\n",
        "T0 = 90      # initial temperature (°C)"
      ],
      "metadata": {
        "id": "KzjTGPdJaZHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "```r\n",
        "Tenv = 20   # ambient temperature (°C)\n",
        "T0   = 90   # initial temperature (°C)\n",
        "\n",
        "cooling %>%\n",
        "  mutate(log_temp = log(temperature - Tenv)) -> cooling\n",
        "model6 = lm(log_temp ~ time, data = cooling)\n",
        "\n",
        "summary(model6)\n",
        "\n",
        "```\n",
        "\n",
        "In this model, the coefficient for `time` is -0.0308977.\n",
        "Therefore, the estimated value of the decay constant $k$ is $\\approx$ 0.0308977.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "YEAo8RvpcyYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 3.2**\n",
        "\n",
        "Perform regression diagnostics using techniques presented in this workshop."
      ],
      "metadata": {
        "id": "M0BAwIYrfbAR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HDpZ7QOico0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "<summary>▶️ Click to show the solution</summary>\n",
        "\n",
        "```r\n",
        "cooling %>%\n",
        "  ggplot(aes(x = time, y = log_temp)) +\n",
        "  geom_point()+\n",
        "  geom_smooth(method = \"lm\")\n",
        "\n",
        "plot(model6, which = 1)\n",
        "plot(model6, which = 2)\n",
        "```\n",
        "\n",
        "As the residuals are scattered around 0, and the scatter plot between `lon_temp` and `time` seems to resemble a strong linear relationship, the linearity assumption appears to be adequate.\n",
        "\n",
        "From the previous two plots, there appears to be a violation of the homoskedasticity assumption: As `time` increases, the spread of residuals around the fitted regression line tends to increase as well.\n",
        "\n",
        "From the Q–Q plot, we can observe a clear violation of the Gaussian assumption, as most points do not lie on the theoretical quantile line. The residuals have fatter tails than standard Gaussian (i.e., tend to produce more extreme values).\n",
        "\n",
        "Overall, the linear regression model can reasonably be applied here, but caution is advised due to the extreme departure from normality and violation of homoskedasticity assumption.\n",
        "\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "TTne9SeLf-gg"
      }
    }
  ]
}